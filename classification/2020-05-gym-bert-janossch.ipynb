{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install huggingface transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (2.8.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (4.42.1)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (2.20.0)\n",
      "Requirement already satisfied: tokenizers==0.5.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (3.0.4)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.6)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (1.12.39)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (2020.4.4)\n",
      "Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (1.15.4)\n",
      "Requirement already satisfied: sentencepiece in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.1.86)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (1.23)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (2.6)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3->transformers) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.39 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3->transformers) (1.15.39)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (1.11.0)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (6.7)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.39->boto3->transformers) (2.7.3)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.39->boto3->transformers) (0.14)\n",
      "\u001b[31mfastai 1.0.60 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: pytorch-nlp in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (0.5.0)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytorch-nlp) (4.42.1)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytorch-nlp) (1.15.4)\n",
      "\u001b[31mfastai 1.0.60 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install pytorch-nlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch                                        # root package\n",
    "from torch.utils.data import Dataset, DataLoader    # dataset representation and loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::733425554560:role/service-role/AmazonSageMaker-ExecutionRole-20200504T094270\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Obtain dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We don't want to use the prepared dataset as it, wo we compare the sample dataset into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aclImdb_v1.tar.gz: 84.1MB [00:12, 6.83MB/s]                              \n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from torchnlp.datasets import imdb_dataset\n",
    "train, test = imdb_dataset(train=True,test=True)\n",
    "\n",
    "with open('data/train.csv', 'w') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile, delimiter=',')\n",
    "    csvwriter.writerow(['text','sentiment'])\n",
    "    for i in train:\n",
    "        csvwriter.writerow([i['text'],i['sentiment']])\n",
    "\n",
    "with open('data/test.csv', 'w') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile, delimiter=',')\n",
    "    csvwriter.writerow(['text','sentiment'])\n",
    "    for i in test:\n",
    "        csvwriter.writerow([i['text'],i['sentiment']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "mapping = {'neg': 0, 'pos': 1}\n",
    "train_df = train_df.replace({'sentiment': mapping})\n",
    "test_df = test_df.replace({'sentiment': mapping})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17837</th>\n",
       "      <td>to make up a movie-going audience - I'm certai...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7450</th>\n",
       "      <td>Following their daughter's brutal murder,Julie...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24797</th>\n",
       "      <td>Where do I start? Per the title of this film I...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4119</th>\n",
       "      <td>Australia's first mainstream slasher film hits...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>Origins of the Care Bears &amp; their Cousins. If ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6139</th>\n",
       "      <td>Tiempo de valientes is a very fun action comed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22520</th>\n",
       "      <td>He really lost the plot with this one! None of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17368</th>\n",
       "      <td>I really liked ZB1. Really, I did. I have no p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1874</th>\n",
       "      <td>The legendary Boris Karloff ended his illustri...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14898</th>\n",
       "      <td>Don't let the name of this film deceive you, I...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  sentiment\n",
       "17837  to make up a movie-going audience - I'm certai...          0\n",
       "7450   Following their daughter's brutal murder,Julie...          1\n",
       "24797  Where do I start? Per the title of this film I...          0\n",
       "4119   Australia's first mainstream slasher film hits...          1\n",
       "1155   Origins of the Care Bears & their Cousins. If ...          1\n",
       "6139   Tiempo de valientes is a very fun action comed...          1\n",
       "22520  He really lost the plot with this one! None of...          0\n",
       "17368  I really liked ZB1. Really, I did. I have no p...          0\n",
       "1874   The legendary Boris Karloff ended his illustri...          1\n",
       "14898  Don't let the name of this film deceive you, I...          0"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18718</th>\n",
       "      <td>Do people rate this movie highly because it's ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12854</th>\n",
       "      <td>Dear friends, I've never seen such a trash mov...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3478</th>\n",
       "      <td>An example of genius filmaking. The epic story...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8403</th>\n",
       "      <td>As a great fan of the Hammer Studios and enthu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17292</th>\n",
       "      <td>Right then. This film is totally unfunny, puer...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4672</th>\n",
       "      <td>i bought this in the budget department last we...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6153</th>\n",
       "      <td>Many of the earlier comments are right on the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5345</th>\n",
       "      <td>The only way to truly understand and relate to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6072</th>\n",
       "      <td>It's a short movie for such immense feelings. ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19651</th>\n",
       "      <td>After huge budget disaster films set in Americ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  sentiment\n",
       "18718  Do people rate this movie highly because it's ...          0\n",
       "12854  Dear friends, I've never seen such a trash mov...          0\n",
       "3478   An example of genius filmaking. The epic story...          1\n",
       "8403   As a great fan of the Hammer Studios and enthu...          1\n",
       "17292  Right then. This film is totally unfunny, puer...          0\n",
       "4672   i bought this in the budget department last we...          1\n",
       "6153   Many of the earlier comments are right on the ...          1\n",
       "5345   The only way to truly understand and relate to...          1\n",
       "6072   It's a short movie for such immense feelings. ...          1\n",
       "19651  After huge budget disaster films set in Americ...          0"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sample(int(len(train_df)*0.1))\n",
    "test_df = test_df.sample(int(len(test_df)*0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = train_df.text.values\n",
    "train_labels = train_df.sentiment.values\n",
    "test_sentences = test_df.text.values\n",
    "test_labels = test_df.sentiment.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "os.makedirs(\"./datasets/train\", exist_ok=True)\n",
    "np.save(\"./datasets/train/train_sentences.npy\", train_sentences)\n",
    "np.save(\"./datasets/train/train_labels.npy\", train_labels)\n",
    "os.makedirs(\"./datasets/test\", exist_ok=True)\n",
    "np.save(\"./datasets/test/test_sentences.npy\", test_sentences)\n",
    "np.save(\"./datasets/test/test_labels.npy\", test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = sess.default_bucket()\n",
    "PREFIX = 'bert-classification-janossch'\n",
    "\n",
    "traindata_s3_prefix = f\"{PREFIX}/datasets/train\"\n",
    "testdata_s3_prefix = f\"{PREFIX}/datasets/test\"\n",
    "output_s3 = f\"s3://{BUCKET_NAME}/{PREFIX}/models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_s3 = sess.upload_data(path=\"./datasets/train/\", bucket=BUCKET_NAME, key_prefix=traindata_s3_prefix)\n",
    "test_s3 = sess.upload_data(path=\"./datasets/test/\", bucket=BUCKET_NAME, key_prefix=testdata_s3_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fbc52bb76a0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: remove - just exploration\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: remove - just exploration\n",
    "\n",
    "# Store the model we want to use\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "\n",
    "# We need to create the model and tokenizer\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['i', 'really', 'liked', 'this', 'summers', '##lam', 'due', 'to', 'the', 'look', 'of', 'the', 'arena', ',', 'the', 'curtains', 'and', 'just', 'the', 'look', 'overall', 'was', 'interesting', 'to', 'me', 'for', 'some', 'reason', '.', 'anyway', '##s', ',', 'this', 'could', 'have', 'been', 'one', 'of', 'the', 'best', 'summers', '##lam', \"'\", 's', 'ever', 'if', 'the', 'wwf', 'didn', \"'\", 't', 'have', 'lex', 'lu', '##ger', 'in', 'the', 'main', 'event', 'against', 'yoko', '##zu', '##na', ',', 'now', 'for', 'it', \"'\", 's', 'time', 'it', 'was', 'ok', 'to', 'have', 'a', 'huge', 'fat', 'man', 'vs', 'a', 'strong', 'man', 'but', 'i', \"'\", 'm', 'glad', 'times', 'have', 'changed', '.', 'it', 'was', 'a', 'terrible', 'main', 'event', 'just', 'like', 'every', 'match', 'lu', '##ger', 'is', 'in', 'is', 'terrible', '.', 'other', 'matches', 'on', 'the', 'card', 'were', 'razor', 'ramon', 'vs', 'ted', 'di', '##bia', '##se', ',', 'steiner', 'brothers', 'vs', 'heavenly', 'bodies', ',', 'shawn', 'michaels', 'vs', 'curt', 'hen', '##ing', ',', 'this', 'was', 'the', 'event', 'where', 'shawn', 'named', 'his', 'big', 'monster', 'of', 'a', 'body', 'guard', 'diesel', ',', 'irs', 'vs', '1', '-', '2', '-', '3', 'kid', ',', 'bret', 'hart', 'first', 'takes', 'on', 'doin', '##k', 'then', 'takes', 'on', 'jerry', 'law', '##ler', 'and', 'stuff', 'with', 'the', 'hart', '##s', 'and', 'law', '##ler', 'was', 'always', 'very', 'interesting', ',', 'then', 'lu', '##d', '##vi', '##g', 'borg', '##a', 'destroyed', 'marty', 'jan', '##nett', '##y', ',', 'undertaker', 'took', 'on', 'giant', 'gonzalez', 'in', 'another', 'terrible', 'match', ',', 'the', 'smoking', 'gunn', '##s', 'and', 'tata', '##nka', 'took', 'on', 'bam', 'bam', 'big', '##elo', '##w', 'and', 'the', 'heads', '##hri', '##nk', '##ers', ',', 'and', 'yoko', '##zu', '##na', 'defended', 'the', 'world', 'title', 'against', 'lex', 'lu', '##ger', 'this', 'match', 'was', 'boring', 'and', 'it', 'has', 'a', 'terrible', 'ending', '.', 'however', 'it', 'deserves', '8', '/', '10']\n"
     ]
    }
   ],
   "source": [
    "# TODO: remove - just exploration\n",
    "\n",
    "tokens = tokenizer.tokenize(train_sentences[0])\n",
    "print(\"Tokens: {}\".format(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "# TODO: remove - just exploration\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  I really liked this Summerslam due to the look of the arena, the curtains and just the look overall was interesting to me for some reason. Anyways, this could have been one of the best Summerslam's ever if the WWF didn't have Lex Luger in the main event against Yokozuna, now for it's time it was ok to have a huge fat man vs a strong man but I'm glad times have changed. It was a terrible main event just like every match Luger is in is terrible. Other matches on the card were Razor Ramon vs Ted Dibiase, Steiner Brothers vs Heavenly Bodies, Shawn Michaels vs Curt Hening, this was the event where Shawn named his big monster of a body guard Diesel, IRS vs 1-2-3 Kid, Bret Hart first takes on Doink then takes on Jerry Lawler and stuff with the Harts and Lawler was always very interesting, then Ludvig Borga destroyed Marty Jannetty, Undertaker took on Giant Gonzalez in another terrible match, The Smoking Gunns and Tatanka took on Bam Bam Bigelow and the Headshrinkers, and Yokozuna defended the world title against Lex Luger this match was boring and it has a terrible ending. However it deserves 8/10\n",
      "Tokenized:  ['i', 'really', 'liked', 'this', 'summers', '##lam', 'due', 'to', 'the', 'look', 'of', 'the', 'arena', ',', 'the', 'curtains', 'and', 'just', 'the', 'look', 'overall', 'was', 'interesting', 'to', 'me', 'for', 'some', 'reason', '.', 'anyway', '##s', ',', 'this', 'could', 'have', 'been', 'one', 'of', 'the', 'best', 'summers', '##lam', \"'\", 's', 'ever', 'if', 'the', 'wwf', 'didn', \"'\", 't', 'have', 'lex', 'lu', '##ger', 'in', 'the', 'main', 'event', 'against', 'yoko', '##zu', '##na', ',', 'now', 'for', 'it', \"'\", 's', 'time', 'it', 'was', 'ok', 'to', 'have', 'a', 'huge', 'fat', 'man', 'vs', 'a', 'strong', 'man', 'but', 'i', \"'\", 'm', 'glad', 'times', 'have', 'changed', '.', 'it', 'was', 'a', 'terrible', 'main', 'event', 'just', 'like', 'every', 'match', 'lu', '##ger', 'is', 'in', 'is', 'terrible', '.', 'other', 'matches', 'on', 'the', 'card', 'were', 'razor', 'ramon', 'vs', 'ted', 'di', '##bia', '##se', ',', 'steiner', 'brothers', 'vs', 'heavenly', 'bodies', ',', 'shawn', 'michaels', 'vs', 'curt', 'hen', '##ing', ',', 'this', 'was', 'the', 'event', 'where', 'shawn', 'named', 'his', 'big', 'monster', 'of', 'a', 'body', 'guard', 'diesel', ',', 'irs', 'vs', '1', '-', '2', '-', '3', 'kid', ',', 'bret', 'hart', 'first', 'takes', 'on', 'doin', '##k', 'then', 'takes', 'on', 'jerry', 'law', '##ler', 'and', 'stuff', 'with', 'the', 'hart', '##s', 'and', 'law', '##ler', 'was', 'always', 'very', 'interesting', ',', 'then', 'lu', '##d', '##vi', '##g', 'borg', '##a', 'destroyed', 'marty', 'jan', '##nett', '##y', ',', 'undertaker', 'took', 'on', 'giant', 'gonzalez', 'in', 'another', 'terrible', 'match', ',', 'the', 'smoking', 'gunn', '##s', 'and', 'tata', '##nka', 'took', 'on', 'bam', 'bam', 'big', '##elo', '##w', 'and', 'the', 'heads', '##hri', '##nk', '##ers', ',', 'and', 'yoko', '##zu', '##na', 'defended', 'the', 'world', 'title', 'against', 'lex', 'lu', '##ger', 'this', 'match', 'was', 'boring', 'and', 'it', 'has', 'a', 'terrible', 'ending', '.', 'however', 'it', 'deserves', '8', '/', '10']\n",
      "Token IDs:  [1045, 2428, 4669, 2023, 10945, 10278, 2349, 2000, 1996, 2298, 1997, 1996, 5196, 1010, 1996, 14694, 1998, 2074, 1996, 2298, 3452, 2001, 5875, 2000, 2033, 2005, 2070, 3114, 1012, 4312, 2015, 1010, 2023, 2071, 2031, 2042, 2028, 1997, 1996, 2190, 10945, 10278, 1005, 1055, 2412, 2065, 1996, 16779, 2134, 1005, 1056, 2031, 17244, 11320, 4590, 1999, 1996, 2364, 2724, 2114, 28758, 9759, 2532, 1010, 2085, 2005, 2009, 1005, 1055, 2051, 2009, 2001, 7929, 2000, 2031, 1037, 4121, 6638, 2158, 5443, 1037, 2844, 2158, 2021, 1045, 1005, 1049, 5580, 2335, 2031, 2904, 1012, 2009, 2001, 1037, 6659, 2364, 2724, 2074, 2066, 2296, 2674, 11320, 4590, 2003, 1999, 2003, 6659, 1012, 2060, 3503, 2006, 1996, 4003, 2020, 15082, 12716, 5443, 6945, 4487, 11607, 3366, 1010, 21264, 3428, 5443, 16581, 4230, 1010, 13218, 17784, 5443, 20099, 21863, 2075, 1010, 2023, 2001, 1996, 2724, 2073, 13218, 2315, 2010, 2502, 6071, 1997, 1037, 2303, 3457, 7937, 1010, 25760, 5443, 1015, 1011, 1016, 1011, 1017, 4845, 1010, 25626, 7530, 2034, 3138, 2006, 24341, 2243, 2059, 3138, 2006, 6128, 2375, 3917, 1998, 4933, 2007, 1996, 7530, 2015, 1998, 2375, 3917, 2001, 2467, 2200, 5875, 1010, 2059, 11320, 2094, 5737, 2290, 28709, 2050, 3908, 12578, 5553, 15361, 2100, 1010, 27568, 2165, 2006, 5016, 10121, 1999, 2178, 6659, 2674, 1010, 1996, 9422, 22079, 2015, 1998, 23236, 25804, 2165, 2006, 25307, 25307, 2502, 18349, 2860, 1998, 1996, 4641, 26378, 8950, 2545, 1010, 1998, 28758, 9759, 2532, 8047, 1996, 2088, 2516, 2114, 17244, 11320, 4590, 2023, 2674, 2001, 11771, 1998, 2009, 2038, 1037, 6659, 4566, 1012, 2174, 2009, 17210, 1022, 1013, 2184]\n"
     ]
    }
   ],
   "source": [
    "# TODO: remove - just exploration\n",
    "\n",
    "print('Original: ',train_sentences[0])\n",
    "print('Tokenized: ',tokenizer.tokenize(texts[0]))\n",
    "print('Token IDs: ',tokenizer.convert_tokens_to_ids(tokenizer.tokenize(train_sentences[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Dataset (moved to the training script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  I really liked this Summerslam due to the look of the arena, the curtains and just the look overall was interesting to me for some reason. Anyways, this could have been one of the best Summerslam's ever if the WWF didn't have Lex Luger in the main event against Yokozuna, now for it's time it was ok to have a huge fat man vs a strong man but I'm glad times have changed. It was a terrible main event just like every match Luger is in is terrible. Other matches on the card were Razor Ramon vs Ted Dibiase, Steiner Brothers vs Heavenly Bodies, Shawn Michaels vs Curt Hening, this was the event where Shawn named his big monster of a body guard Diesel, IRS vs 1-2-3 Kid, Bret Hart first takes on Doink then takes on Jerry Lawler and stuff with the Harts and Lawler was always very interesting, then Ludvig Borga destroyed Marty Jannetty, Undertaker took on Giant Gonzalez in another terrible match, The Smoking Gunns and Tatanka took on Bam Bam Bigelow and the Headshrinkers, and Yokozuna defended the world title against Lex Luger this match was boring and it has a terrible ending. However it deserves 8/10\n",
      "Token IDs:  tensor([  101,  1045,  2428,  4669,  2023, 10945, 10278,  2349,  2000,  1996,\n",
      "         2298,  1997,  1996,  5196,  1010,  1996, 14694,  1998,  2074,  1996,\n",
      "         2298,  3452,  2001,  5875,  2000,  2033,  2005,  2070,  3114,  1012,\n",
      "         4312,  2015,  1010,  2023,  2071,  2031,  2042,  2028,  1997,  1996,\n",
      "         2190, 10945, 10278,  1005,  1055,  2412,  2065,  1996, 16779,   102])\n"
     ]
    }
   ],
   "source": [
    "# TODO: remove - just exploration\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for t in train_sentences:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        t,\n",
    "                        add_special_tokens = True,\n",
    "                        max_length = 50,\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt',\n",
    "                   )\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_mask = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(train_labels)\n",
    "\n",
    "print('Original: ', train_sentences[0])\n",
    "print('Token IDs: ', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: remove - just exploration\n",
    "from transformers import BertModel\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Set the model in evaluation mode to deactivate the DropOut modules\n",
    "# This is IMPORTANT to have reproducible results during evaluation!\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU, put everything on cuda\n",
    "input_ids = input_ids.to('cuda')\n",
    "labels = labels.to('cuda')\n",
    "model.to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_channel = f\"s3://{BUCKET_NAME}/{PREFIX}/datasets/train/\"\n",
    "test_channel = f\"s3://{BUCKET_NAME}/{PREFIX}/datasets/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.estimator import PyTorch as PyTorchEstimator\n",
    "estimator = PyTorchEstimator(\n",
    "    entry_point=\"janossch-train.py\",\n",
    "    source_dir=\"src\",\n",
    "    \n",
    "    base_job_name=\"bert-classification\",\n",
    "    output_path=f\"s3://{BUCKET_NAME}/{PREFIX}/\",\n",
    "    \n",
    "    framework_version=\"1.4.0\",\n",
    "    py_version=\"py3\",\n",
    "    \n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=\"local\",\n",
    "    train_max_run=60*60,\n",
    "    train_max_wait=60*60,\n",
    "    \n",
    "    hyperparameters={\n",
    "        \"seed\": 4711,\n",
    "        \"log_level\": \"DEBUG\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating tmp70lj25zk_algo-1-x170w_1 ... \n",
      "\u001b[1BAttaching to tmp70lj25zk_algo-1-x170w_12mdone\u001b[0m\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m 2020-05-05 09:28:31,417 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m 2020-05-05 09:28:31,421 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m 2020-05-05 09:28:31,435 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m 2020-05-05 09:28:31,437 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m 2020-05-05 09:28:31,566 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m Generating setup.py\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m 2020-05-05 09:28:31,567 sagemaker-containers INFO     Generating setup.cfg\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m 2020-05-05 09:28:31,567 sagemaker-containers INFO     Generating MANIFEST.in\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m 2020-05-05 09:28:31,567 sagemaker-containers INFO     Installing module with the following command:\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m /opt/conda/bin/python -m pip install . -r requirements.txt\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m Processing /tmp/tmpz7vnhjbo/module_dir\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m Collecting transformers\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m   Downloading transformers-2.8.0-py3-none-any.whl (563 kB)\n",
      "\u001b[K     |████████████████████████████████| 563 kB 10.8 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m \u001b[?25hRequirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 1)) (1.12.34)\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m Collecting sentencepiece\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m   Downloading sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 12.1 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m \u001b[?25hRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 1)) (4.42.1)\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 1)) (2.22.0)\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m Collecting regex!=2019.12.17\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m   Downloading regex-2020.4.4-cp36-cp36m-manylinux2010_x86_64.whl (679 kB)\n",
      "\u001b[K     |████████████████████████████████| 679 kB 37.5 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m \u001b[?25hCollecting filelock\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m   Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m Requirement already satisfied: dataclasses; python_version < \"3.7\" in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 1)) (0.7)\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m Collecting sacremoses\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m   Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
      "\u001b[K     |████████████████████████████████| 883 kB 42.5 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m \u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 1)) (1.16.4)\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m Collecting tokenizers==0.5.2\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m   Downloading tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.7 MB 36.8 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m \u001b[?25hRequirement already satisfied: botocore<1.16.0,>=1.15.34 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers->-r requirements.txt (line 1)) (1.15.34)\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers->-r requirements.txt (line 1)) (0.9.5)\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers->-r requirements.txt (line 1)) (0.3.3)\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 1)) (3.0.4)\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 1)) (1.25.8)\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 1)) (2.8)\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 1)) (2019.11.28)\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers->-r requirements.txt (line 1)) (1.14.0)\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers->-r requirements.txt (line 1)) (7.1.1)\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers->-r requirements.txt (line 1)) (0.14.1)\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.34->boto3->transformers->-r requirements.txt (line 1)) (2.8.1)\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m Requirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.34->boto3->transformers->-r requirements.txt (line 1)) (0.15.2)\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m Building wheels for collected packages: default-user-module-name, sacremoses\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m   Building wheel for default-user-module-name (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m \u001b[?25h  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=18791 sha256=d4b1a2199abd08372889c1197c37fb8a40d0d3d3778deefe7f643a6184e4d4e9\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m   Stored in directory: /tmp/pip-ephem-wheel-cache-0gjnl4kp/wheels/f1/6f/81/f842e5752f1645441940556fe102b39d1239c27aa0fb293cf4\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m   Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m \u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893259 sha256=6407f0c34715e9b8fadaa68b3054493d87ba7352df15d84812dc6ca9016b8fed\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m   Stored in directory: /root/.cache/pip/wheels/49/25/98/cdea9c79b2d9a22ccc59540b1784b67f06b633378e97f58da2\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m Successfully built default-user-module-name sacremoses\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m Installing collected packages: sentencepiece, regex, filelock, sacremoses, tokenizers, transformers, default-user-module-name\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m Successfully installed default-user-module-name-1.0.0 filelock-3.0.12 regex-2020.4.4 sacremoses-0.0.43 sentencepiece-0.1.86 tokenizers-0.5.2 transformers-2.8.0\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m 2020-05-05 09:28:36,746 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m 2020-05-05 09:28:36,763 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m 2020-05-05 09:28:36,780 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m 2020-05-05 09:28:36,795 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m \n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m Training Env:\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m \n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m {\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m     \"additional_framework_parameters\": {},\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m     \"channel_input_dirs\": {\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m         \"train\": \"/opt/ml/input/data/train\",\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m         \"test\": \"/opt/ml/input/data/test\"\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m     \"current_host\": \"algo-1-x170w\",\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m     \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m     \"hosts\": [\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m         \"algo-1-x170w\"\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m     ],\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m         \"seed\": 4711,\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m         \"log_level\": \"DEBUG\"\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m     \"input_data_config\": {\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m         \"train\": {\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m         },\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m         \"test\": {\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m         }\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m     \"job_name\": \"bert-classification-2020-05-05-09-28-28-799\",\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m     \"master_hostname\": \"algo-1-x170w\",\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m     \"module_dir\": \"s3://sagemaker-ap-southeast-1-733425554560/bert-classification-2020-05-05-09-28-28-799/source/sourcedir.tar.gz\",\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m     \"module_name\": \"janossch-train\",\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m     \"num_cpus\": 4,\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m     \"num_gpus\": 0,\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m         \"current_host\": \"algo-1-x170w\",\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m         \"hosts\": [\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m             \"algo-1-x170w\"\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m         ]\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m     \"user_entry_point\": \"janossch-train.py\"\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m }\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m \n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m Environment variables:\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m \n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m SM_HOSTS=[\"algo-1-x170w\"]\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m SM_HPS={\"log_level\":\"DEBUG\",\"seed\":4711}\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m SM_USER_ENTRY_POINT=janossch-train.py\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m SM_FRAMEWORK_PARAMS={}\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-x170w\",\"hosts\":[\"algo-1-x170w\"]}\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m SM_INPUT_DATA_CONFIG={\"test\":{\"TrainingInputMode\":\"File\"},\"train\":{\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m SM_CHANNELS=[\"test\",\"train\"]\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m SM_CURRENT_HOST=algo-1-x170w\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m SM_MODULE_NAME=janossch-train\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m SM_NUM_CPUS=4\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m SM_NUM_GPUS=0\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m SM_MODULE_DIR=s3://sagemaker-ap-southeast-1-733425554560/bert-classification-2020-05-05-09-28-28-799/source/sourcedir.tar.gz\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1-x170w\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1-x170w\"],\"hyperparameters\":{\"log_level\":\"DEBUG\",\"seed\":4711},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"TrainingInputMode\":\"File\"},\"train\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"bert-classification-2020-05-05-09-28-28-799\",\"log_level\":20,\"master_hostname\":\"algo-1-x170w\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-southeast-1-733425554560/bert-classification-2020-05-05-09-28-28-799/source/sourcedir.tar.gz\",\"module_name\":\"janossch-train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-x170w\",\"hosts\":[\"algo-1-x170w\"]},\"user_entry_point\":\"janossch-train.py\"}\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m SM_USER_ARGS=[\"--log_level\",\"DEBUG\",\"--seed\",\"4711\"]\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m SM_CHANNEL_TEST=/opt/ml/input/data/test\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m SM_HP_SEED=4711\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m SM_HP_LOG_LEVEL=DEBUG\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m \n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m \n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m /opt/conda/bin/python janossch-train.py --log_level DEBUG --seed 4711\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m \n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36malgo-1-x170w_1  |\u001b[0m No GPU available, using the CPU instead.\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m \n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m ======== Epoch 1 / 4 ========\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m Training...\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m   Batch    40  of     79.    Elapsed: 0:05:02.\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m \n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m   Average training loss: 0.59\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m   Training epcoh took: 0:09:49\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m \n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m Running Validation...\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m   Accuracy: 0.77\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m   Validation Loss: 0.02\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m   Validation took: 0:02:58\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m \n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m ======== Epoch 2 / 4 ========\n",
      "\u001b[36malgo-1-x170w_1  |\u001b[0m Training...\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({ \"train\": train_channel, \"test\": test_channel })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
