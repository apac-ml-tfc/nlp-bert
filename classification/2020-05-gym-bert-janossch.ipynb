{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install huggingface transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (2.8.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (4.42.1)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (2.20.0)\n",
      "Requirement already satisfied: tokenizers==0.5.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (3.0.4)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.6)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (1.12.39)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (2020.4.4)\n",
      "Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (1.15.4)\n",
      "Requirement already satisfied: sentencepiece in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.1.86)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (1.23)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (2.6)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3->transformers) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.39 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3->transformers) (1.15.39)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (1.11.0)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (6.7)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.39->boto3->transformers) (2.7.3)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.39->boto3->transformers) (0.14)\n",
      "\u001b[31mfastai 1.0.60 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: pytorch-nlp in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (0.5.0)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytorch-nlp) (4.42.1)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytorch-nlp) (1.15.4)\n",
      "\u001b[31mfastai 1.0.60 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install pytorch-nlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch                                        # root package\n",
    "from torch.utils.data import Dataset, DataLoader    # dataset representation and loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::733425554560:role/service-role/AmazonSageMaker-ExecutionRole-20200504T094270\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Obtain dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We don't want to use the prepared dataset as it, wo we compare the sample dataset into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aclImdb_v1.tar.gz: 84.1MB [00:12, 6.83MB/s]                              \n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from torchnlp.datasets import imdb_dataset\n",
    "train, test = imdb_dataset(train=True,test=True)\n",
    "\n",
    "with open('data/train.csv', 'w') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile, delimiter=',')\n",
    "    csvwriter.writerow(['text','sentiment'])\n",
    "    for i in train:\n",
    "        csvwriter.writerow([i['text'],i['sentiment']])\n",
    "\n",
    "with open('data/test.csv', 'w') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile, delimiter=',')\n",
    "    csvwriter.writerow(['text','sentiment'])\n",
    "    for i in test:\n",
    "        csvwriter.writerow([i['text'],i['sentiment']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "mapping = {'neg': -1, 'pos': 1}\n",
    "train_df = train_df.replace({'sentiment': mapping})\n",
    "test_df = test_df.replace({'sentiment': mapping})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3069</th>\n",
       "      <td>This film, was one of my childhood favorites a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>Gillian Holroyd (Kim Novak) is a witch. Secret...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17094</th>\n",
       "      <td>When I first tuned in on this morning news, I ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2586</th>\n",
       "      <td>I have NOT seen this movie, but I must. Having...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14280</th>\n",
       "      <td>They had an opportunity to make one of the bes...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7235</th>\n",
       "      <td>You better see this episode from the beginning...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23309</th>\n",
       "      <td>This movie is so awful, it is hard to find the...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9145</th>\n",
       "      <td>After reviewing this intense martial arts movi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18571</th>\n",
       "      <td>While I have never been a fan of the original ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13372</th>\n",
       "      <td>This movie is BAD! It's basically an overdone ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  sentiment\n",
       "3069   This film, was one of my childhood favorites a...          1\n",
       "896    Gillian Holroyd (Kim Novak) is a witch. Secret...          1\n",
       "17094  When I first tuned in on this morning news, I ...         -1\n",
       "2586   I have NOT seen this movie, but I must. Having...          1\n",
       "14280  They had an opportunity to make one of the bes...         -1\n",
       "7235   You better see this episode from the beginning...          1\n",
       "23309  This movie is so awful, it is hard to find the...         -1\n",
       "9145   After reviewing this intense martial arts movi...          1\n",
       "18571  While I have never been a fan of the original ...         -1\n",
       "13372  This movie is BAD! It's basically an overdone ...         -1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20378</th>\n",
       "      <td>and I still don't know where the hell this mov...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9756</th>\n",
       "      <td>If you are looking for a modern film version o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17608</th>\n",
       "      <td>this is an example of a movie that can have gr...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3402</th>\n",
       "      <td>This movie is certainly one of the greatest fi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21590</th>\n",
       "      <td>What a travesty of movie ratings injustice - a...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18771</th>\n",
       "      <td>Absolutely horrible movie. Not a bad plot conc...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7891</th>\n",
       "      <td>This is an account of events that have been co...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12238</th>\n",
       "      <td>Its a feel-good movie that made me feel good. ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7580</th>\n",
       "      <td>I wish I was first exposed to this in a movie ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3593</th>\n",
       "      <td>I'm sure the film contains certain gaps in log...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  sentiment\n",
       "20378  and I still don't know where the hell this mov...         -1\n",
       "9756   If you are looking for a modern film version o...          1\n",
       "17608  this is an example of a movie that can have gr...         -1\n",
       "3402   This movie is certainly one of the greatest fi...          1\n",
       "21590  What a travesty of movie ratings injustice - a...         -1\n",
       "18771  Absolutely horrible movie. Not a bad plot conc...         -1\n",
       "7891   This is an account of events that have been co...          1\n",
       "12238  Its a feel-good movie that made me feel good. ...          1\n",
       "7580   I wish I was first exposed to this in a movie ...          1\n",
       "3593   I'm sure the film contains certain gaps in log...          1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sample(int(len(train_df)*0.1))\n",
    "test_df = test_df.sample(int(len(test_df)*0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = train_df.text.values\n",
    "train_labels = train_df.sentiment.values\n",
    "test_sentences = test_df.text.values\n",
    "test_labels = test_df.sentiment.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "os.makedirs(\"./datasets/train\", exist_ok=True)\n",
    "np.save(\"./datasets/train/train_sentences.npy\", train_sentences)\n",
    "np.save(\"./datasets/train/train_labels.npy\", train_labels)\n",
    "os.makedirs(\"./datasets/test\", exist_ok=True)\n",
    "np.save(\"./datasets/test/test_sentences.npy\", test_sentences)\n",
    "np.save(\"./datasets/test/test_labels.npy\", test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = sess.default_bucket()\n",
    "PREFIX = 'bert-classification-janossch'\n",
    "\n",
    "traindata_s3_prefix = f\"{PREFIX}/datasets/train\"\n",
    "testdata_s3_prefix = f\"{PREFIX}/datasets/test\"\n",
    "output_s3 = f\"s3://{BUCKET_NAME}/{PREFIX}/models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_s3 = sess.upload_data(path=\"./datasets/train/\", bucket=BUCKET_NAME, key_prefix=traindata_s3_prefix)\n",
    "test_s3 = sess.upload_data(path=\"./datasets/test/\", bucket=BUCKET_NAME, key_prefix=testdata_s3_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fbc52bb76a0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: remove - just exploration\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: remove - just exploration\n",
    "\n",
    "# Store the model we want to use\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "\n",
    "# We need to create the model and tokenizer\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['i', 'really', 'liked', 'this', 'summers', '##lam', 'due', 'to', 'the', 'look', 'of', 'the', 'arena', ',', 'the', 'curtains', 'and', 'just', 'the', 'look', 'overall', 'was', 'interesting', 'to', 'me', 'for', 'some', 'reason', '.', 'anyway', '##s', ',', 'this', 'could', 'have', 'been', 'one', 'of', 'the', 'best', 'summers', '##lam', \"'\", 's', 'ever', 'if', 'the', 'wwf', 'didn', \"'\", 't', 'have', 'lex', 'lu', '##ger', 'in', 'the', 'main', 'event', 'against', 'yoko', '##zu', '##na', ',', 'now', 'for', 'it', \"'\", 's', 'time', 'it', 'was', 'ok', 'to', 'have', 'a', 'huge', 'fat', 'man', 'vs', 'a', 'strong', 'man', 'but', 'i', \"'\", 'm', 'glad', 'times', 'have', 'changed', '.', 'it', 'was', 'a', 'terrible', 'main', 'event', 'just', 'like', 'every', 'match', 'lu', '##ger', 'is', 'in', 'is', 'terrible', '.', 'other', 'matches', 'on', 'the', 'card', 'were', 'razor', 'ramon', 'vs', 'ted', 'di', '##bia', '##se', ',', 'steiner', 'brothers', 'vs', 'heavenly', 'bodies', ',', 'shawn', 'michaels', 'vs', 'curt', 'hen', '##ing', ',', 'this', 'was', 'the', 'event', 'where', 'shawn', 'named', 'his', 'big', 'monster', 'of', 'a', 'body', 'guard', 'diesel', ',', 'irs', 'vs', '1', '-', '2', '-', '3', 'kid', ',', 'bret', 'hart', 'first', 'takes', 'on', 'doin', '##k', 'then', 'takes', 'on', 'jerry', 'law', '##ler', 'and', 'stuff', 'with', 'the', 'hart', '##s', 'and', 'law', '##ler', 'was', 'always', 'very', 'interesting', ',', 'then', 'lu', '##d', '##vi', '##g', 'borg', '##a', 'destroyed', 'marty', 'jan', '##nett', '##y', ',', 'undertaker', 'took', 'on', 'giant', 'gonzalez', 'in', 'another', 'terrible', 'match', ',', 'the', 'smoking', 'gunn', '##s', 'and', 'tata', '##nka', 'took', 'on', 'bam', 'bam', 'big', '##elo', '##w', 'and', 'the', 'heads', '##hri', '##nk', '##ers', ',', 'and', 'yoko', '##zu', '##na', 'defended', 'the', 'world', 'title', 'against', 'lex', 'lu', '##ger', 'this', 'match', 'was', 'boring', 'and', 'it', 'has', 'a', 'terrible', 'ending', '.', 'however', 'it', 'deserves', '8', '/', '10']\n"
     ]
    }
   ],
   "source": [
    "# TODO: remove - just exploration\n",
    "\n",
    "tokens = tokenizer.tokenize(train_sentences[0])\n",
    "print(\"Tokens: {}\".format(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "# TODO: remove - just exploration\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  I really liked this Summerslam due to the look of the arena, the curtains and just the look overall was interesting to me for some reason. Anyways, this could have been one of the best Summerslam's ever if the WWF didn't have Lex Luger in the main event against Yokozuna, now for it's time it was ok to have a huge fat man vs a strong man but I'm glad times have changed. It was a terrible main event just like every match Luger is in is terrible. Other matches on the card were Razor Ramon vs Ted Dibiase, Steiner Brothers vs Heavenly Bodies, Shawn Michaels vs Curt Hening, this was the event where Shawn named his big monster of a body guard Diesel, IRS vs 1-2-3 Kid, Bret Hart first takes on Doink then takes on Jerry Lawler and stuff with the Harts and Lawler was always very interesting, then Ludvig Borga destroyed Marty Jannetty, Undertaker took on Giant Gonzalez in another terrible match, The Smoking Gunns and Tatanka took on Bam Bam Bigelow and the Headshrinkers, and Yokozuna defended the world title against Lex Luger this match was boring and it has a terrible ending. However it deserves 8/10\n",
      "Tokenized:  ['i', 'really', 'liked', 'this', 'summers', '##lam', 'due', 'to', 'the', 'look', 'of', 'the', 'arena', ',', 'the', 'curtains', 'and', 'just', 'the', 'look', 'overall', 'was', 'interesting', 'to', 'me', 'for', 'some', 'reason', '.', 'anyway', '##s', ',', 'this', 'could', 'have', 'been', 'one', 'of', 'the', 'best', 'summers', '##lam', \"'\", 's', 'ever', 'if', 'the', 'wwf', 'didn', \"'\", 't', 'have', 'lex', 'lu', '##ger', 'in', 'the', 'main', 'event', 'against', 'yoko', '##zu', '##na', ',', 'now', 'for', 'it', \"'\", 's', 'time', 'it', 'was', 'ok', 'to', 'have', 'a', 'huge', 'fat', 'man', 'vs', 'a', 'strong', 'man', 'but', 'i', \"'\", 'm', 'glad', 'times', 'have', 'changed', '.', 'it', 'was', 'a', 'terrible', 'main', 'event', 'just', 'like', 'every', 'match', 'lu', '##ger', 'is', 'in', 'is', 'terrible', '.', 'other', 'matches', 'on', 'the', 'card', 'were', 'razor', 'ramon', 'vs', 'ted', 'di', '##bia', '##se', ',', 'steiner', 'brothers', 'vs', 'heavenly', 'bodies', ',', 'shawn', 'michaels', 'vs', 'curt', 'hen', '##ing', ',', 'this', 'was', 'the', 'event', 'where', 'shawn', 'named', 'his', 'big', 'monster', 'of', 'a', 'body', 'guard', 'diesel', ',', 'irs', 'vs', '1', '-', '2', '-', '3', 'kid', ',', 'bret', 'hart', 'first', 'takes', 'on', 'doin', '##k', 'then', 'takes', 'on', 'jerry', 'law', '##ler', 'and', 'stuff', 'with', 'the', 'hart', '##s', 'and', 'law', '##ler', 'was', 'always', 'very', 'interesting', ',', 'then', 'lu', '##d', '##vi', '##g', 'borg', '##a', 'destroyed', 'marty', 'jan', '##nett', '##y', ',', 'undertaker', 'took', 'on', 'giant', 'gonzalez', 'in', 'another', 'terrible', 'match', ',', 'the', 'smoking', 'gunn', '##s', 'and', 'tata', '##nka', 'took', 'on', 'bam', 'bam', 'big', '##elo', '##w', 'and', 'the', 'heads', '##hri', '##nk', '##ers', ',', 'and', 'yoko', '##zu', '##na', 'defended', 'the', 'world', 'title', 'against', 'lex', 'lu', '##ger', 'this', 'match', 'was', 'boring', 'and', 'it', 'has', 'a', 'terrible', 'ending', '.', 'however', 'it', 'deserves', '8', '/', '10']\n",
      "Token IDs:  [1045, 2428, 4669, 2023, 10945, 10278, 2349, 2000, 1996, 2298, 1997, 1996, 5196, 1010, 1996, 14694, 1998, 2074, 1996, 2298, 3452, 2001, 5875, 2000, 2033, 2005, 2070, 3114, 1012, 4312, 2015, 1010, 2023, 2071, 2031, 2042, 2028, 1997, 1996, 2190, 10945, 10278, 1005, 1055, 2412, 2065, 1996, 16779, 2134, 1005, 1056, 2031, 17244, 11320, 4590, 1999, 1996, 2364, 2724, 2114, 28758, 9759, 2532, 1010, 2085, 2005, 2009, 1005, 1055, 2051, 2009, 2001, 7929, 2000, 2031, 1037, 4121, 6638, 2158, 5443, 1037, 2844, 2158, 2021, 1045, 1005, 1049, 5580, 2335, 2031, 2904, 1012, 2009, 2001, 1037, 6659, 2364, 2724, 2074, 2066, 2296, 2674, 11320, 4590, 2003, 1999, 2003, 6659, 1012, 2060, 3503, 2006, 1996, 4003, 2020, 15082, 12716, 5443, 6945, 4487, 11607, 3366, 1010, 21264, 3428, 5443, 16581, 4230, 1010, 13218, 17784, 5443, 20099, 21863, 2075, 1010, 2023, 2001, 1996, 2724, 2073, 13218, 2315, 2010, 2502, 6071, 1997, 1037, 2303, 3457, 7937, 1010, 25760, 5443, 1015, 1011, 1016, 1011, 1017, 4845, 1010, 25626, 7530, 2034, 3138, 2006, 24341, 2243, 2059, 3138, 2006, 6128, 2375, 3917, 1998, 4933, 2007, 1996, 7530, 2015, 1998, 2375, 3917, 2001, 2467, 2200, 5875, 1010, 2059, 11320, 2094, 5737, 2290, 28709, 2050, 3908, 12578, 5553, 15361, 2100, 1010, 27568, 2165, 2006, 5016, 10121, 1999, 2178, 6659, 2674, 1010, 1996, 9422, 22079, 2015, 1998, 23236, 25804, 2165, 2006, 25307, 25307, 2502, 18349, 2860, 1998, 1996, 4641, 26378, 8950, 2545, 1010, 1998, 28758, 9759, 2532, 8047, 1996, 2088, 2516, 2114, 17244, 11320, 4590, 2023, 2674, 2001, 11771, 1998, 2009, 2038, 1037, 6659, 4566, 1012, 2174, 2009, 17210, 1022, 1013, 2184]\n"
     ]
    }
   ],
   "source": [
    "# TODO: remove - just exploration\n",
    "\n",
    "print('Original: ',train_sentences[0])\n",
    "print('Tokenized: ',tokenizer.tokenize(texts[0]))\n",
    "print('Token IDs: ',tokenizer.convert_tokens_to_ids(tokenizer.tokenize(train_sentences[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Dataset (moved to the training script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  I really liked this Summerslam due to the look of the arena, the curtains and just the look overall was interesting to me for some reason. Anyways, this could have been one of the best Summerslam's ever if the WWF didn't have Lex Luger in the main event against Yokozuna, now for it's time it was ok to have a huge fat man vs a strong man but I'm glad times have changed. It was a terrible main event just like every match Luger is in is terrible. Other matches on the card were Razor Ramon vs Ted Dibiase, Steiner Brothers vs Heavenly Bodies, Shawn Michaels vs Curt Hening, this was the event where Shawn named his big monster of a body guard Diesel, IRS vs 1-2-3 Kid, Bret Hart first takes on Doink then takes on Jerry Lawler and stuff with the Harts and Lawler was always very interesting, then Ludvig Borga destroyed Marty Jannetty, Undertaker took on Giant Gonzalez in another terrible match, The Smoking Gunns and Tatanka took on Bam Bam Bigelow and the Headshrinkers, and Yokozuna defended the world title against Lex Luger this match was boring and it has a terrible ending. However it deserves 8/10\n",
      "Token IDs:  tensor([  101,  1045,  2428,  4669,  2023, 10945, 10278,  2349,  2000,  1996,\n",
      "         2298,  1997,  1996,  5196,  1010,  1996, 14694,  1998,  2074,  1996,\n",
      "         2298,  3452,  2001,  5875,  2000,  2033,  2005,  2070,  3114,  1012,\n",
      "         4312,  2015,  1010,  2023,  2071,  2031,  2042,  2028,  1997,  1996,\n",
      "         2190, 10945, 10278,  1005,  1055,  2412,  2065,  1996, 16779,   102])\n"
     ]
    }
   ],
   "source": [
    "# TODO: remove - just exploration\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for t in train_sentences:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        t,\n",
    "                        add_special_tokens = True,\n",
    "                        max_length = 50,\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt',\n",
    "                   )\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_mask = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(train_labels)\n",
    "\n",
    "print('Original: ', train_sentences[0])\n",
    "print('Token IDs: ', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: remove - just exploration\n",
    "from transformers import BertModel\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Set the model in evaluation mode to deactivate the DropOut modules\n",
    "# This is IMPORTANT to have reproducible results during evaluation!\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU, put everything on cuda\n",
    "input_ids = input_ids.to('cuda')\n",
    "labels = labels.to('cuda')\n",
    "model.to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_channel = f\"s3://{BUCKET_NAME}/{PREFIX}/datasets/train/\"\n",
    "test_channel = f\"s3://{BUCKET_NAME}/{PREFIX}/datasets/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.estimator import PyTorch as PyTorchEstimator\n",
    "estimator = PyTorchEstimator(\n",
    "    entry_point=\"janossch-train.py\",\n",
    "    source_dir=\"src\",\n",
    "    \n",
    "    base_job_name=\"bert-classification\",\n",
    "    output_path=f\"s3://{BUCKET_NAME}/{PREFIX}/\",\n",
    "    \n",
    "    framework_version=\"1.4.0\",\n",
    "    py_version=\"py3\",\n",
    "    \n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=\"local\",\n",
    "    train_max_run=60*60,\n",
    "    train_max_wait=60*60,\n",
    "    \n",
    "    hyperparameters={\n",
    "        \"seed\": 4711,\n",
    "        \"log_level\": \"DEBUG\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating tmpx_cw6v92_algo-1-53b4j_1 ... \n",
      "\u001b[1BAttaching to tmpx_cw6v92_algo-1-53b4j_12mdone\u001b[0m\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m 2020-05-05 08:04:26,271 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m 2020-05-05 08:04:26,274 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m 2020-05-05 08:04:26,287 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m 2020-05-05 08:04:26,290 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m 2020-05-05 08:04:26,431 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m Generating setup.py\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m 2020-05-05 08:04:26,431 sagemaker-containers INFO     Generating setup.cfg\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m 2020-05-05 08:04:26,431 sagemaker-containers INFO     Generating MANIFEST.in\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m 2020-05-05 08:04:26,431 sagemaker-containers INFO     Installing module with the following command:\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m /opt/conda/bin/python -m pip install . -r requirements.txt\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m Processing /tmp/tmpe4hbu62z/module_dir\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m Collecting transformers\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m   Downloading transformers-2.8.0-py3-none-any.whl (563 kB)\n",
      "\u001b[K     |████████████████████████████████| 563 kB 16.3 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m \u001b[?25hCollecting filelock\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m   Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 1)) (4.42.1)\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m Requirement already satisfied: dataclasses; python_version < \"3.7\" in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 1)) (0.7)\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 1)) (1.16.4)\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m Collecting sentencepiece\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m   Downloading sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 40.8 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m \u001b[?25hCollecting regex!=2019.12.17\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m   Downloading regex-2020.4.4-cp36-cp36m-manylinux2010_x86_64.whl (679 kB)\n",
      "\u001b[K     |████████████████████████████████| 679 kB 51.0 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m \u001b[?25hCollecting tokenizers==0.5.2\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m   Downloading tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.7 MB 45.8 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m \u001b[?25hRequirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 1)) (1.12.34)\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 1)) (2.22.0)\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m Collecting sacremoses\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m   Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
      "\u001b[K     |████████████████████████████████| 883 kB 43.5 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m \u001b[?25hRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers->-r requirements.txt (line 1)) (0.9.5)\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers->-r requirements.txt (line 1)) (0.3.3)\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m Requirement already satisfied: botocore<1.16.0,>=1.15.34 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers->-r requirements.txt (line 1)) (1.15.34)\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 1)) (1.25.8)\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 1)) (2019.11.28)\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 1)) (3.0.4)\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 1)) (2.8)\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers->-r requirements.txt (line 1)) (1.14.0)\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers->-r requirements.txt (line 1)) (7.1.1)\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers->-r requirements.txt (line 1)) (0.14.1)\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m Requirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.34->boto3->transformers->-r requirements.txt (line 1)) (0.15.2)\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.34->boto3->transformers->-r requirements.txt (line 1)) (2.8.1)\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m Building wheels for collected packages: default-user-module-name, sacremoses\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m   Building wheel for default-user-module-name (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m \u001b[?25h  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=16322 sha256=b71efa2e97f4c9b75d2fb134463d293d371cff40e273211c42043a16e55af06f\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m   Stored in directory: /tmp/pip-ephem-wheel-cache-vi481q0p/wheels/2e/a8/2f/e0c721797c96a62a43fd510371d62c30a566238f5c4378d85d\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m   Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m \u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893259 sha256=78834786e101487def7e360ba89f9da70e29ea61c7717eb29edaaf5560592c63\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m   Stored in directory: /root/.cache/pip/wheels/49/25/98/cdea9c79b2d9a22ccc59540b1784b67f06b633378e97f58da2\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m Successfully built default-user-module-name sacremoses\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m Installing collected packages: filelock, sentencepiece, regex, tokenizers, sacremoses, transformers, default-user-module-name\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m Successfully installed default-user-module-name-1.0.0 filelock-3.0.12 regex-2020.4.4 sacremoses-0.0.43 sentencepiece-0.1.86 tokenizers-0.5.2 transformers-2.8.0\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m 2020-05-05 08:04:31,553 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m 2020-05-05 08:04:31,569 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m 2020-05-05 08:04:31,585 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m 2020-05-05 08:04:31,600 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m \n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m Training Env:\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m \n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m {\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m     \"additional_framework_parameters\": {},\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m     \"channel_input_dirs\": {\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m         \"train\": \"/opt/ml/input/data/train\",\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m         \"test\": \"/opt/ml/input/data/test\"\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m     \"current_host\": \"algo-1-53b4j\",\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m     \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m     \"hosts\": [\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m         \"algo-1-53b4j\"\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m     ],\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m         \"seed\": 4711,\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m         \"log_level\": \"DEBUG\"\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m     \"input_data_config\": {\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m         \"train\": {\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m         },\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m         \"test\": {\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m         }\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m     \"job_name\": \"bert-classification-2020-05-05-08-04-23-690\",\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m     \"master_hostname\": \"algo-1-53b4j\",\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m     \"module_dir\": \"s3://sagemaker-ap-southeast-1-733425554560/bert-classification-2020-05-05-08-04-23-690/source/sourcedir.tar.gz\",\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m     \"module_name\": \"janossch-train\",\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m     \"num_cpus\": 4,\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m     \"num_gpus\": 0,\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m         \"current_host\": \"algo-1-53b4j\",\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m         \"hosts\": [\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m             \"algo-1-53b4j\"\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m         ]\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m     \"user_entry_point\": \"janossch-train.py\"\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m }\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m \n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m Environment variables:\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m \n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m SM_HOSTS=[\"algo-1-53b4j\"]\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m SM_HPS={\"log_level\":\"DEBUG\",\"seed\":4711}\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m SM_USER_ENTRY_POINT=janossch-train.py\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m SM_FRAMEWORK_PARAMS={}\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-53b4j\",\"hosts\":[\"algo-1-53b4j\"]}\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m SM_INPUT_DATA_CONFIG={\"test\":{\"TrainingInputMode\":\"File\"},\"train\":{\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m SM_CHANNELS=[\"test\",\"train\"]\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m SM_CURRENT_HOST=algo-1-53b4j\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m SM_MODULE_NAME=janossch-train\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m SM_NUM_CPUS=4\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m SM_NUM_GPUS=0\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m SM_MODULE_DIR=s3://sagemaker-ap-southeast-1-733425554560/bert-classification-2020-05-05-08-04-23-690/source/sourcedir.tar.gz\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1-53b4j\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1-53b4j\"],\"hyperparameters\":{\"log_level\":\"DEBUG\",\"seed\":4711},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"TrainingInputMode\":\"File\"},\"train\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"bert-classification-2020-05-05-08-04-23-690\",\"log_level\":20,\"master_hostname\":\"algo-1-53b4j\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-southeast-1-733425554560/bert-classification-2020-05-05-08-04-23-690/source/sourcedir.tar.gz\",\"module_name\":\"janossch-train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-53b4j\",\"hosts\":[\"algo-1-53b4j\"]},\"user_entry_point\":\"janossch-train.py\"}\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m SM_USER_ARGS=[\"--log_level\",\"DEBUG\",\"--seed\",\"4711\"]\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m SM_CHANNEL_TEST=/opt/ml/input/data/test\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m SM_HP_SEED=4711\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m SM_HP_LOG_LEVEL=DEBUG\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m \n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m \n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m /opt/conda/bin/python janossch-train.py --log_level DEBUG --seed 4711\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m \n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m 2020-05-05 08:06:23,843 sagemaker-containers ERROR    ExecuteUserScriptError:\r\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m Command \"/opt/conda/bin/python janossch-train.py --log_level DEBUG --seed 4711\"\r\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m INFO:train:Starting!\r\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m DEBUG:train:Namespace(base_job_name='bert-classification-2020-05-05-08-04-23-690', base_model_name='bert-base-uncased', batch_size=64, epochs=4, learning_rate=0.05, log_level='DEBUG', model_dir='/opt/ml/model', num_gpus=0, num_labels=2, output_data_dir='/opt/ml/output/data', output_mode='classification', seed=4711, test='/opt/ml/input/data/test', train='/opt/ml/input/data/train', use_cuda=False)\r\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m DEBUG:train:training function\r\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m DEBUG:train:setting seed: Namespace(base_job_name='bert-classification-2020-05-05-08-04-23-690', base_model_name='bert-base-uncased', batch_size=64, epochs=4, learning_rate=0.05, log_level='DEBUG', model_dir='/opt/ml/model', num_gpus=0, num_labels=2, output_data_dir='/opt/ml/output/data', output_mode='classification', seed=4711, test='/opt/ml/input/data/test', train='/opt/ml/input/data/train', use_cuda=False)\r\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m DEBUG:train:num_labels: 2\r\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m DEBUG:train:output_mode: classification\r\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m \r",
      "Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]\r",
      "Downloading: 100%|ââââââââââ| 433/433 [00:00<00:00, 408kB/s]\r\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m \r",
      "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]\r",
      "Downloading:   8%|â         | 17.4k/232k [00:00<00:02, 76.1kB/s]\r",
      "Downloading:  23%|âââ       | 52.2k/232k [00:00<00:02, 89.5kB/s]\r",
      "Downloading:  53%|ââââââ    | 122k/232k [00:00<00:00, 113kB/s]  \r",
      "Downloading:  98%|ââââââââââ| 226k/232k [00:00<00:00, 146kB/s]\r",
      "Downloading: 100%|ââââââââââ| 232k/232k [00:00<00:00, 252kB/s]\r\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m \r",
      "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]\r",
      "Downloading:   0%|          | 17.4k/440M [00:00<1:40:36, 73.0kB/s]\r",
      "Downloading:   0%|          | 52.2k/440M [00:00<1:25:34, 85.8kB/s]\r",
      "Downloading:   0%|          | 122k/440M [00:00<1:07:27, 109kB/s]  \r",
      "Downloading:   0%|          | 243k/440M [00:00<51:34, 142kB/s]  \r",
      "Downloading:   0%|          | 400k/440M [00:01<39:25, 186kB/s]\r",
      "Downloading:   0%|          | 749k/440M [00:01<29:05, 252kB/s]\r",
      "Downloading:   0%|          | 1.08M/440M [00:01<21:56, 334kB/s]\r",
      "Downloading:   0%|          | 1.88M/440M [00:01<15:59, 457kB/s]\r",
      "Downloading:   1%|          | 3.21M/440M [00:02<11:32, 631kB/s]\r",
      "Downloading:   1%|          | 4.79M/440M [00:02<08:23, 866kB/s]\r",
      "Downloading:   1%|â         | 6.38M/440M [00:02<06:10, 1.17MB/s]\r",
      "Downloading:   2%|â         | 7.97M/440M [00:02<04:37, 1.56MB/s]\r",
      "Downloading:   2%|â         | 9.54M/440M [00:03<03:33, 2.02MB/s]\r",
      "Downloading:   3%|â         | 11.1M/440M [00:03<02:48, 2.55MB/s]\r",
      "Downloading:   3%|â         | 12.7M/440M [00:03<02:16, 3.13MB/s]\r",
      "Downloading:   3%|â         | 14.3M/440M [00:03<01:54, 3.71MB/s]\r",
      "Downloading:   4%|â         | 15.9M/440M [00:04<01:39, 4.28MB/s]\r",
      "Downloading:   4%|â         | 17.5M/440M [00:04<01:28, 4.79MB/s]\r",
      "Downloading:   4%|â         | 19.0M/440M [00:04<01:20, 5.21MB/s]\r",
      "Downloading:   5%|â         | 20.6M/440M [00:04<01:15, 5.57MB/s]\r",
      "Downloading:   5%|â         | 22.2M/440M [00:05<01:11, 5.84MB/s]\r",
      "Downloading:   5%|â         | 23.8M/440M [00:05<01:08, 6.06MB/s]\r",
      "Downloading:   6%|â         | 25.4M/440M [00:05<01:06, 6.22MB/s]\r",
      "Downloading:   6%|â         | 27.0M/440M [00:05<01:05, 6.32MB/s]\r",
      "Downloading:   6%|â         | 28.5M/440M [00:05<01:04, 6.41MB/s]\r",
      "Downloading:   7%|â         | 30.1M/440M [00:06<01:03, 6.48MB/s]\r",
      "Downloading:   7%|â         | 31.7M/440M [00:06<01:02, 6.50MB/s]\r",
      "Downloading:   8%|â         | 33.3M/440M [00:06<01:02, 6.54MB/s]\r",
      "Downloading:   8%|â         | 34.9M/440M [00:06<01:01, 6.57MB/s]\r",
      "Downloading:   8%|â         | 36.5M/440M [00:07<01:01, 6.57MB/s]\r",
      "Downloading:   9%|â         | 38.0M/440M [00:07<01:01, 6.59MB/s]\r",
      "Downloading:   9%|â         | 39.6M/440M [00:07<01:00, 6.60MB/s]\r",
      "Downloading:   9%|â         | 41.2M/440M [00:07<01:00, 6.59MB/s]\r",
      "Downloading:  10%|â         | 42.8M/440M [00:08<01:00, 6.60MB/s]\r",
      "Downloading:  10%|â         | 44.4M/440M [00:08<00:59, 6.61MB/s]\r",
      "Downloading:  10%|â         | 46.0M/440M [00:08<00:59, 6.60MB/s]\r",
      "Downloading:  11%|â         | 47.5M/440M [00:08<00:59, 6.61MB/s]\r",
      "Downloading:  11%|â         | 49.1M/440M [00:09<00:59, 6.62MB/s]\r",
      "Downloading:  12%|ââ        | 50.7M/440M [00:09<00:59, 6.60MB/s]\r",
      "Downloading:  12%|ââ        | 52.3M/440M [00:09<00:58, 6.61MB/s]\r",
      "Downloading:  12%|ââ        | 53.9M/440M [00:09<00:58, 6.62MB/s]\r",
      "Downloading:  13%|ââ        | 55.5M/440M [00:10<00:58, 6.60MB/s]\r",
      "Downloading:  13%|ââ        | 57.1M/440M [00:10<00:57, 6.61MB/s]\r",
      "Downloading:  13%|ââ        | 58.6M/440M [00:10<00:57, 6.60MB/s]\r",
      "Downloading:  14%|ââ        | 60.2M/440M [00:10<00:57, 6.61MB/s]\r",
      "Downloading:  14%|ââ        | 61.8M/440M [00:11<00:57, 6.62MB/s]\r",
      "Downloading:  14%|ââ        | 63.4M/440M [00:11<00:57, 6.60MB/s]\r",
      "Downloading:  15%|ââ        | 65.0M/440M [00:11<00:56, 6.61MB/s]\r",
      "Downloading:  15%|ââ        | 66.6M/440M [00:11<00:56, 6.62MB/s]\r",
      "Downloading:  15%|ââ        | 68.1M/440M [00:11<00:56, 6.60MB/s]\r",
      "Downloading:  16%|ââ        | 69.7M/440M [00:12<00:56, 6.61MB/s]\r",
      "Downloading:  16%|ââ        | 71.3M/440M [00:12<00:55, 6.62MB/s]\r",
      "Downloading:  17%|ââ        | 72.9M/440M [00:12<00:55, 6.60MB/s]\r",
      "Downloading:  17%|ââ        | 74.5M/440M [00:12<00:55, 6.61MB/s]\r",
      "Downloading:  17%|ââ        | 76.1M/440M [00:13<00:55, 6.62MB/s]\r",
      "Downloading:  18%|ââ        | 77.6M/440M [00:13<00:54, 6.60MB/s]\r",
      "Downloading:  18%|ââ        | 79.2M/440M [00:13<00:54, 6.61MB/s]\r",
      "Downloading:  18%|ââ        | 80.8M/440M [00:13<00:54, 6.62MB/s]\r",
      "Downloading:  19%|ââ        | 82.4M/440M [00:14<00:54, 6.60MB/s]\r",
      "Downloading:  19%|ââ        | 84.0M/440M [00:14<00:53, 6.61MB/s]\r",
      "Downloading:  19%|ââ        | 85.6M/440M [00:14<00:53, 6.62MB/s]\r",
      "Downloading:  20%|ââ        | 87.1M/440M [00:14<00:53, 6.60MB/s]\r",
      "Downloading:  20%|ââ        | 88.7M/440M [00:15<00:53, 6.61MB/s]\r",
      "Downloading:  21%|ââ        | 90.3M/440M [00:15<00:52, 6.62MB/s]\r",
      "Downloading:  21%|ââ        | 91.9M/440M [00:15<00:52, 6.60MB/s]\r",
      "Downloading:  21%|ââ        | 93.5M/440M [00:15<00:52, 6.61MB/s]\r",
      "Downloading:  22%|âââ       | 95.1M/440M [00:16<00:52, 6.61MB/s]\r",
      "Downloading:  22%|âââ       | 96.6M/440M [00:16<00:52, 6.61MB/s]\r",
      "Downloading:  22%|âââ       | 98.2M/440M [00:16<00:51, 6.61MB/s]\r",
      "Downloading:  23%|âââ       | 99.8M/440M [00:16<00:51, 6.62MB/s]\r",
      "Downloading:  23%|âââ       | 101M/440M [00:17<00:51, 6.61MB/s] \r",
      "Downloading:  23%|âââ       | 103M/440M [00:17<00:51, 6.62MB/s]\r",
      "Downloading:  24%|âââ       | 105M/440M [00:17<00:50, 6.62MB/s]\r",
      "Downloading:  24%|âââ       | 106M/440M [00:17<00:50, 6.60MB/s]\r",
      "Downloading:  24%|âââ       | 108M/440M [00:17<00:50, 6.61MB/s]\r",
      "Downloading:  25%|âââ       | 109M/440M [00:18<00:50, 6.62MB/s]\r",
      "Downloading:  25%|âââ       | 111M/440M [00:18<00:49, 6.61MB/s]\r",
      "Downloading:  26%|âââ       | 112M/440M [00:18<00:49, 6.61MB/s]\r",
      "Downloading:  26%|âââ       | 114M/440M [00:18<00:49, 6.62MB/s]\r",
      "Downloading:  26%|âââ       | 116M/440M [00:19<00:49, 6.61MB/s]\r",
      "Downloading:  27%|âââ       | 117M/440M [00:19<00:48, 6.61MB/s]\r",
      "Downloading:  27%|âââ       | 119M/440M [00:19<00:48, 6.62MB/s]\r",
      "Downloading:  27%|âââ       | 120M/440M [00:19<00:48, 6.61MB/s]\r",
      "Downloading:  28%|âââ       | 122M/440M [00:20<00:48, 6.61MB/s]\r",
      "Downloading:  28%|âââ       | 124M/440M [00:20<00:47, 6.62MB/s]\r",
      "Downloading:  28%|âââ       | 125M/440M [00:20<00:47, 6.61MB/s]\r",
      "Downloading:  29%|âââ       | 127M/440M [00:20<00:47, 6.61MB/s]\r",
      "Downloading:  29%|âââ       | 128M/440M [00:21<00:47, 6.62MB/s]\r",
      "Downloading:  29%|âââ       | 130M/440M [00:21<00:47, 6.61MB/s]\r",
      "Downloading:  30%|âââ       | 131M/440M [00:21<00:46, 6.61MB/s]\r",
      "Downloading:  30%|âââ       | 133M/440M [00:21<00:46, 6.62MB/s]\r",
      "Downloading:  31%|âââ       | 135M/440M [00:22<00:46, 6.61MB/s]\r",
      "Downloading:  31%|âââ       | 136M/440M [00:22<00:46, 6.61MB/s]\r",
      "Downloading:  31%|ââââ      | 138M/440M [00:22<00:45, 6.60MB/s]\r",
      "Downloading:  32%|ââââ      | 139M/440M [00:22<00:45, 6.60MB/s]\r",
      "Downloading:  32%|ââââ      | 141M/440M [00:22<00:45, 6.61MB/s]\r",
      "Downloading:  32%|ââââ      | 143M/440M [00:23<00:45, 6.61MB/s]\r",
      "Downloading:  33%|ââââ      | 144M/440M [00:23<00:44, 6.61MB/s]\r",
      "Downloading:  33%|ââââ      | 146M/440M [00:23<00:44, 6.62MB/s]\r",
      "Downloading:  33%|ââââ      | 147M/440M [00:23<00:44, 6.61MB/s]\r",
      "Downloading:  34%|ââââ      | 149M/440M [00:24<00:44, 6.61MB/s]\r",
      "Downloading:  34%|ââââ      | 150M/440M [00:24<00:43, 6.62MB/s]\r",
      "Downloading:  35%|ââââ      | 152M/440M [00:24<00:43, 6.61MB/s]\r",
      "Downloading:  35%|ââââ      | 154M/440M [00:24<00:43, 6.61MB/s]\r",
      "Downloading:  35%|ââââ      | 155M/440M [00:25<00:41, 6.81MB/s]\r",
      "Downloading:  36%|ââââ      | 157M/440M [00:25<00:43, 6.56MB/s]\r",
      "Downloading:  36%|ââââ      | 158M/440M [00:25<00:42, 6.58MB/s]\r",
      "Downloading:  36%|ââââ      | 160M/440M [00:25<00:42, 6.58MB/s]\r",
      "Downloading:  37%|ââââ      | 162M/440M [00:26<00:42, 6.59MB/s]\r",
      "Downloading:  37%|ââââ      | 163M/440M [00:26<00:42, 6.60MB/s]\r",
      "Downloading:  37%|ââââ      | 165M/440M [00:26<00:41, 6.60MB/s]\r",
      "Downloading:  38%|ââââ      | 166M/440M [00:26<00:41, 6.61MB/s]\r",
      "Downloading:  38%|ââââ      | 168M/440M [00:27<00:41, 6.61MB/s]\r",
      "Downloading:  38%|ââââ      | 169M/440M [00:27<00:41, 6.60MB/s]\r",
      "Downloading:  39%|ââââ      | 171M/440M [00:27<00:40, 6.61MB/s]\r",
      "Downloading:  39%|ââââ      | 173M/440M [00:27<00:40, 6.62MB/s]\r",
      "Downloading:  40%|ââââ      | 174M/440M [00:28<00:40, 6.60MB/s]\r",
      "Downloading:  40%|ââââ      | 176M/440M [00:28<00:40, 6.61MB/s]\r",
      "Downloading:  40%|ââââ      | 177M/440M [00:28<00:39, 6.62MB/s]\r",
      "Downloading:  41%|ââââ      | 179M/440M [00:28<00:39, 6.60MB/s]\r",
      "Downloading:  41%|ââââ      | 181M/440M [00:28<00:39, 6.61MB/s]\r",
      "Downloading:  41%|âââââ     | 182M/440M [00:29<00:39, 6.62MB/s]\r",
      "Downloading:  42%|âââââ     | 184M/440M [00:29<00:38, 6.61MB/s]\r",
      "Downloading:  42%|âââââ     | 185M/440M [00:29<00:38, 6.61MB/s]\r",
      "Downloading:  42%|âââââ     | 187M/440M [00:29<00:38, 6.62MB/s]\r",
      "Downloading:  43%|âââââ     | 188M/440M [00:30<00:38, 6.61MB/s]\r",
      "Downloading:  43%|âââââ     | 190M/440M [00:30<00:37, 6.61MB/s]\r",
      "Downloading:  44%|âââââ     | 192M/440M [00:30<00:37, 6.60MB/s]\r",
      "Downloading:  44%|âââââ     | 193M/440M [00:30<00:37, 6.61MB/s]\r",
      "Downloading:  44%|âââââ     | 195M/440M [00:31<00:37, 6.61MB/s]\r",
      "Downloading:  45%|âââââ     | 196M/440M [00:31<00:36, 6.60MB/s]\r",
      "Downloading:  45%|âââââ     | 198M/440M [00:31<00:36, 6.61MB/s]\r",
      "Downloading:  45%|âââââ     | 200M/440M [00:31<00:36, 6.61MB/s]\r",
      "Downloading:  46%|âââââ     | 201M/440M [00:32<00:36, 6.61MB/s]\r",
      "Downloading:  46%|âââââ     | 203M/440M [00:32<00:35, 6.62MB/s]\r",
      "Downloading:  46%|âââââ     | 204M/440M [00:32<00:35, 6.62MB/s]\r",
      "Downloading:  47%|âââââ     | 206M/440M [00:32<00:35, 6.61MB/s]\r",
      "Downloading:  47%|âââââ     | 207M/440M [00:33<00:35, 6.61MB/s]\r",
      "Downloading:  47%|âââââ     | 209M/440M [00:33<00:32, 7.17MB/s]\r",
      "Downloading:  48%|âââââ     | 210M/440M [00:33<00:28, 7.95MB/s]\r",
      "Downloading:  48%|âââââ     | 211M/440M [00:33<00:36, 6.26MB/s]\r",
      "Downloading:  48%|âââââ     | 212M/440M [00:33<00:39, 5.79MB/s]\r",
      "Downloading:  49%|âââââ     | 214M/440M [00:34<00:37, 6.01MB/s]\r",
      "Downloading:  49%|âââââ     | 215M/440M [00:34<00:36, 6.18MB/s]\r",
      "Downloading:  49%|âââââ     | 217M/440M [00:34<00:35, 6.31MB/s]\r",
      "Downloading:  50%|âââââ     | 219M/440M [00:34<00:34, 6.39MB/s]\r",
      "Downloading:  50%|âââââ     | 220M/440M [00:34<00:34, 6.46MB/s]\r",
      "Downloading:  50%|âââââ     | 222M/440M [00:35<00:33, 6.51MB/s]\r",
      "Downloading:  51%|âââââ     | 223M/440M [00:35<00:33, 6.53MB/s]\r",
      "Downloading:  51%|âââââ     | 225M/440M [00:35<00:32, 6.56MB/s]\r",
      "Downloading:  51%|ââââââ    | 226M/440M [00:35<00:32, 6.58MB/s]\r",
      "Downloading:  52%|ââââââ    | 228M/440M [00:36<00:32, 6.58MB/s]\r",
      "Downloading:  52%|ââââââ    | 230M/440M [00:36<00:31, 6.60MB/s]\r",
      "Downloading:  52%|ââââââ    | 231M/440M [00:36<00:31, 6.61MB/s]\r",
      "Downloading:  53%|ââââââ    | 233M/440M [00:36<00:31, 6.60MB/s]\r",
      "Downloading:  53%|ââââââ    | 234M/440M [00:37<00:31, 6.61MB/s]\r",
      "Downloading:  54%|ââââââ    | 236M/440M [00:37<00:30, 6.62MB/s]\r",
      "Downloading:  54%|ââââââ    | 238M/440M [00:37<00:30, 6.60MB/s]\r",
      "Downloading:  54%|ââââââ    | 239M/440M [00:37<00:30, 6.61MB/s]\r",
      "Downloading:  55%|ââââââ    | 241M/440M [00:38<00:30, 6.62MB/s]\r",
      "Downloading:  55%|ââââââ    | 242M/440M [00:38<00:30, 6.60MB/s]\r",
      "Downloading:  55%|ââââââ    | 244M/440M [00:38<00:29, 6.61MB/s]\r",
      "Downloading:  56%|ââââââ    | 246M/440M [00:38<00:29, 6.62MB/s]\r",
      "Downloading:  56%|ââââââ    | 247M/440M [00:39<00:29, 6.61MB/s]\r",
      "Downloading:  56%|ââââââ    | 249M/440M [00:39<00:29, 6.61MB/s]\r",
      "Downloading:  57%|ââââââ    | 250M/440M [00:39<00:28, 6.79MB/s]\r",
      "Downloading:  57%|ââââââ    | 252M/440M [00:39<00:28, 6.56MB/s]\r",
      "Downloading:  58%|ââââââ    | 253M/440M [00:39<00:28, 6.58MB/s]\r",
      "Downloading:  58%|ââââââ    | 255M/440M [00:40<00:28, 6.58MB/s]\r",
      "Downloading:  58%|ââââââ    | 257M/440M [00:40<00:27, 6.59MB/s]\r",
      "Downloading:  59%|ââââââ    | 258M/440M [00:40<00:27, 6.61MB/s]\r",
      "Downloading:  59%|ââââââ    | 260M/440M [00:40<00:27, 6.60MB/s]\r",
      "Downloading:  59%|ââââââ    | 261M/440M [00:41<00:27, 6.61MB/s]\r",
      "Downloading:  60%|ââââââ    | 263M/440M [00:41<00:26, 6.62MB/s]\r",
      "Downloading:  60%|ââââââ    | 264M/440M [00:41<00:40, 4.37MB/s]\r",
      "Downloading:  60%|ââââââ    | 266M/440M [00:41<00:33, 5.17MB/s]\r",
      "Downloading:  61%|ââââââ    | 267M/440M [00:42<00:36, 4.80MB/s]\r",
      "Downloading:  61%|ââââââ    | 268M/440M [00:42<00:33, 5.22MB/s]\r",
      "Downloading:  61%|ââââââ    | 270M/440M [00:42<00:31, 5.47MB/s]\r",
      "Downloading:  62%|âââââââ   | 271M/440M [00:42<00:29, 5.74MB/s]\r",
      "Downloading:  62%|âââââââ   | 273M/440M [00:43<00:28, 5.82MB/s]\r",
      "Downloading:  62%|âââââââ   | 274M/440M [00:43<00:27, 6.01MB/s]\r",
      "Downloading:  63%|âââââââ   | 276M/440M [00:43<00:27, 6.01MB/s]\r",
      "Downloading:  63%|âââââââ   | 277M/440M [00:43<00:26, 6.17MB/s]\r",
      "Downloading:  63%|âââââââ   | 279M/440M [00:44<00:26, 6.12MB/s]\r",
      "Downloading:  64%|âââââââ   | 280M/440M [00:44<00:25, 6.25MB/s]\r",
      "Downloading:  64%|âââââââ   | 282M/440M [00:44<00:25, 6.24MB/s]\r",
      "Downloading:  64%|âââââââ   | 283M/440M [00:44<00:24, 6.34MB/s]\r",
      "Downloading:  65%|âââââââ   | 285M/440M [00:45<00:24, 6.24MB/s]\r",
      "Downloading:  65%|âââââââ   | 286M/440M [00:45<00:24, 6.33MB/s]\r",
      "Downloading:  65%|âââââââ   | 288M/440M [00:45<00:24, 6.24MB/s]\r",
      "Downloading:  66%|âââââââ   | 289M/440M [00:45<00:23, 6.33MB/s]\r",
      "Downloading:  66%|âââââââ   | 291M/440M [00:45<00:24, 6.19MB/s]\r",
      "Downloading:  66%|âââââââ   | 292M/440M [00:46<00:23, 6.32MB/s]\r",
      "Downloading:  67%|âââââââ   | 294M/440M [00:46<00:23, 6.29MB/s]\r",
      "Downloading:  67%|âââââââ   | 295M/440M [00:46<00:22, 6.35MB/s]\r",
      "Downloading:  67%|âââââââ   | 297M/440M [00:46<00:23, 6.20MB/s]\r",
      "Downloading:  68%|âââââââ   | 298M/440M [00:47<00:22, 6.33MB/s]\r",
      "Downloading:  68%|âââââââ   | 300M/440M [00:47<00:22, 6.21MB/s]\r",
      "Downloading:  68%|âââââââ   | 301M/440M [00:47<00:22, 6.31MB/s]\r",
      "Downloading:  69%|âââââââ   | 303M/440M [00:47<00:21, 6.26MB/s]\r",
      "Downloading:  69%|âââââââ   | 304M/440M [00:48<00:21, 6.35MB/s]\r",
      "Downloading:  69%|âââââââ   | 306M/440M [00:48<00:21, 6.27MB/s]\r",
      "Downloading:  70%|âââââââ   | 307M/440M [00:48<00:20, 6.34MB/s]\r",
      "Downloading:  70%|âââââââ   | 309M/440M [00:48<00:21, 6.26MB/s]\r",
      "Downloading:  70%|âââââââ   | 310M/440M [00:49<00:20, 6.35MB/s]\r",
      "Downloading:  71%|âââââââ   | 312M/440M [00:49<00:20, 6.25MB/s]\r",
      "Downloading:  71%|âââââââ   | 313M/440M [00:49<00:20, 6.34MB/s]\r",
      "Downloading:  72%|ââââââââ  | 315M/440M [00:49<00:19, 6.41MB/s]\r",
      "Downloading:  72%|ââââââââ  | 317M/440M [00:50<00:19, 6.47MB/s]\r",
      "Downloading:  72%|ââââââââ  | 318M/440M [00:50<00:18, 6.50MB/s]\r",
      "Downloading:  73%|ââââââââ  | 320M/440M [00:50<00:18, 6.52MB/s]\r",
      "Downloading:  73%|ââââââââ  | 321M/440M [00:50<00:18, 6.55MB/s]\r",
      "Downloading:  73%|ââââââââ  | 323M/440M [00:51<00:17, 6.56MB/s]\r",
      "Downloading:  74%|ââââââââ  | 324M/440M [00:51<00:17, 6.56MB/s]\r",
      "Downloading:  74%|ââââââââ  | 326M/440M [00:51<00:17, 6.58MB/s]\r",
      "Downloading:  74%|ââââââââ  | 328M/440M [00:51<00:17, 6.58MB/s]\r",
      "Downloading:  75%|ââââââââ  | 329M/440M [00:51<00:16, 6.58MB/s]\r",
      "Downloading:  75%|ââââââââ  | 331M/440M [00:52<00:16, 6.59MB/s]\r",
      "Downloading:  75%|ââââââââ  | 332M/440M [00:52<00:16, 6.58MB/s]\r",
      "Downloading:  76%|ââââââââ  | 334M/440M [00:52<00:16, 6.58MB/s]\r",
      "Downloading:  76%|ââââââââ  | 336M/440M [00:52<00:15, 6.58MB/s]\r",
      "Downloading:  77%|ââââââââ  | 337M/440M [00:53<00:15, 6.59MB/s]\r",
      "Downloading:  77%|ââââââââ  | 339M/440M [00:53<00:15, 6.59MB/s]\r",
      "Downloading:  77%|ââââââââ  | 340M/440M [00:53<00:15, 6.58MB/s]\r",
      "Downloading:  78%|ââââââââ  | 342M/440M [00:53<00:14, 6.60MB/s]\r",
      "Downloading:  78%|ââââââââ  | 343M/440M [00:54<00:14, 6.59MB/s]\r",
      "Downloading:  78%|ââââââââ  | 345M/440M [00:54<00:14, 6.58MB/s]\r",
      "Downloading:  79%|ââââââââ  | 347M/440M [00:54<00:14, 6.57MB/s]\r",
      "Downloading:  79%|ââââââââ  | 348M/440M [00:54<00:14, 6.59MB/s]\r",
      "Downloading:  79%|ââââââââ  | 350M/440M [00:55<00:13, 6.59MB/s]\r",
      "Downloading:  80%|ââââââââ  | 351M/440M [00:55<00:13, 6.58MB/s]\r",
      "Downloading:  80%|ââââââââ  | 353M/440M [00:55<00:13, 6.60MB/s]\r",
      "Downloading:  80%|ââââââââ  | 354M/440M [00:55<00:13, 6.59MB/s]\r",
      "Downloading:  81%|ââââââââ  | 356M/440M [00:56<00:12, 6.58MB/s]\r",
      "Downloading:  81%|ââââââââ  | 358M/440M [00:56<00:10, 7.57MB/s]\r",
      "Downloading:  81%|âââââââââ | 358M/440M [00:56<00:11, 7.35MB/s]\r",
      "Downloading:  82%|âââââââââ | 359M/440M [00:56<00:14, 5.48MB/s]\r",
      "Downloading:  82%|âââââââââ | 361M/440M [00:56<00:13, 5.77MB/s]\r",
      "Downloading:  82%|âââââââââ | 362M/440M [00:57<00:13, 5.99MB/s]\r",
      "Downloading:  83%|âââââââââ | 364M/440M [00:57<00:12, 6.17MB/s]\r",
      "Downloading:  83%|âââââââââ | 365M/440M [00:57<00:11, 6.28MB/s]\r",
      "Downloading:  83%|âââââââââ | 367M/440M [00:57<00:11, 6.37MB/s]\r",
      "Downloading:  84%|âââââââââ | 369M/440M [00:57<00:11, 6.44MB/s]\r",
      "Downloading:  84%|âââââââââ | 370M/440M [00:58<00:10, 6.48MB/s]\r",
      "Downloading:  84%|âââââââââ | 372M/440M [00:58<00:10, 6.51MB/s]\r",
      "Downloading:  85%|âââââââââ | 373M/440M [00:58<00:09, 6.72MB/s]\r",
      "Downloading:  85%|âââââââââ | 375M/440M [00:58<00:08, 7.82MB/s]\r",
      "Downloading:  85%|âââââââââ | 376M/440M [00:58<00:09, 6.83MB/s]\r",
      "Downloading:  85%|âââââââââ | 377M/440M [00:59<00:11, 5.56MB/s]\r",
      "Downloading:  86%|âââââââââ | 378M/440M [00:59<00:10, 5.83MB/s]\r",
      "Downloading:  86%|âââââââââ | 380M/440M [00:59<00:10, 6.05MB/s]\r",
      "Downloading:  87%|âââââââââ | 381M/440M [00:59<00:09, 6.20MB/s]\r",
      "Downloading:  87%|âââââââââ | 383M/440M [01:00<00:09, 6.30MB/s]\r",
      "Downloading:  87%|âââââââââ | 384M/440M [01:00<00:08, 6.39MB/s]\r",
      "Downloading:  88%|âââââââââ | 386M/440M [01:00<00:08, 6.45MB/s]\r",
      "Downloading:  88%|âââââââââ | 388M/440M [01:00<00:08, 6.49MB/s]\r",
      "Downloading:  88%|âââââââââ | 389M/440M [01:01<00:07, 6.52MB/s]\r",
      "Downloading:  89%|ââââ��âââââ | 391M/440M [01:01<00:07, 6.54MB/s]\r",
      "Downloading:  89%|âââââââââ | 392M/440M [01:01<00:07, 6.55MB/s]\r",
      "Downloading:  89%|âââââââââ | 394M/440M [01:01<00:06, 7.54MB/s]\r",
      "Downloading:  90%|âââââââââ | 395M/440M [01:01<00:06, 7.15MB/s]\r",
      "Downloading:  90%|âââââââââ | 395M/440M [01:02<00:08, 5.51MB/s]\r",
      "Downloading:  90%|âââââââââ | 397M/440M [01:02<00:07, 5.79MB/s]\r",
      "Downloading:  90%|âââââââââ | 399M/440M [01:02<00:06, 6.00MB/s]\r",
      "Downloading:  91%|âââââââââ | 400M/440M [01:02<00:06, 6.18MB/s]\r",
      "Downloading:  91%|âââââââââ | 402M/440M [01:03<00:06, 6.29MB/s]\r",
      "Downloading:  92%|ââââââââââ| 403M/440M [01:03<00:05, 6.37MB/s]\r",
      "Downloading:  92%|ââââââââââ| 405M/440M [01:03<00:05, 6.43MB/s]\r",
      "Downloading:  92%|ââââââââââ| 407M/440M [01:03<00:05, 6.49MB/s]\r",
      "Downloading:  93%|ââââââââââ| 408M/440M [01:03<00:04, 6.51MB/s]\r",
      "Downloading:  93%|ââââââââââ| 410M/440M [01:04<00:04, 6.53MB/s]\r",
      "Downloading:  93%|ââââââââââ| 411M/440M [01:04<00:04, 6.56MB/s]\r",
      "Downloading:  94%|ââââââââââ| 413M/440M [01:04<00:04, 6.56MB/s]\r",
      "Downloading:  94%|ââââââââââ| 414M/440M [01:04<00:03, 6.56MB/s]\r",
      "Downloading:  94%|ââââââââââ| 416M/440M [01:05<00:03, 6.59MB/s]\r",
      "Downloading:  95%|ââââââââââ| 418M/440M [01:05<00:03, 6.58MB/s]\r",
      "Downloading:  95%|ââââââââââ| 419M/440M [01:05<00:03, 6.58MB/s]\r",
      "Downloading:  96%|ââââââââââ| 421M/440M [01:05<00:03, 6.55MB/s]\r",
      "Downloading:  96%|ââââââââââ| 422M/440M [01:06<00:02, 6.55MB/s]\r",
      "Downloading:  96%|ââââââââââ| 424M/440M [01:06<00:02, 6.56MB/s]\r",
      "Downloading:  97%|ââââââââââ| 425M/440M [01:06<00:02, 6.56MB/s]\r",
      "Downloading:  97%|ââââââââââ| 427M/440M [01:06<00:02, 6.59MB/s]\r",
      "Downloading:  97%|ââââââââââ| 429M/440M [01:07<00:01, 6.58MB/s]\r",
      "Downloading:  98%|ââââââââââ| 430M/440M [01:07<00:01, 6.58MB/s]\r",
      "Downloading:  98%|ââââââââââ| 432M/440M [01:07<00:01, 6.60MB/s]\r",
      "Downloading:  98%|ââââââââââ| 433M/440M [01:07<00:01, 6.58MB/s]\r",
      "Downloading:  99%|ââââââââââ| 435M/440M [01:08<00:00, 6.58MB/s]\r",
      "Downloading:  99%|ââââââââââ| 437M/440M [01:08<00:00, 6.59MB/s]\r",
      "Downloading:  99%|ââââââââââ| 438M/440M [01:08<00:00, 6.59MB/s]\r",
      "Downloading: 100%|ââââââââââ| 440M/440M [01:08<00:00, 6.58MB/s]\r",
      "Downloading: 100%|ââââââââââ| 440M/440M [01:08<00:00, 6.40MB/s]\r\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m DEBUG:train:train set: /opt/ml/input/data/train\r\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m DEBUG:train:test set: /opt/ml/input/data/test\r\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m DEBUG:train:loading files as numpy array\r\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m DEBUG:train:starting with tokenizing the training sentences\r\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m DEBUG:train:finished with tokenizing the training sentences\r\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m DEBUG:train:starting with tokenizing the test sentences\r\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m DEBUG:train:finished with tokenizing the test sentences\r\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m DEBUG:train:Original: Doyle had never wanted to resurrect Holmes from his joint death with Professor Moriarty in THE ADVENTURE OF THE FINAL PROBLEM. However,financial considerations made him willing (in 1901) to write THE HOUND OF THE BASKERVILLES, which is still considered his best Holmes' novel and possibly his best novel. But it was a \"memoir\" of the great detective, written before his death. Only a greater outcry from his public led Doyle to fully resurrect Holmes in THE ADVENTURE OF THE EMPTY HOUSE, published in 1905.<br /><br />It is not that the new short stories (and the last novel) are really bad. Maybe three of the stories are really terrible, but even the terrible ones are very readable. Several of the later ones (like THE ADVENTURE OF THE SOLITARY CYCLIST) are really very good. But the unevenness of production (in particularly after the stories in HIS LAST BOW (1917)) become increasingly apparent. He repeats past story lines, and he shows really negative aspects of Holmes. In the story THE ADVENTURE OF THE THREE GABLES Holmes shows a sneering sarcasm at a character who is of African ancestry. <br /><br />SPOILER COMING UP:<br /><br />THE ADVENTURE OF CHARLES AUGUSTUS MILVERTON deals with Holmes trying to recover compromising letters from Milverton, a hugely successful blackmailer. It is an interesting example of how Doyle could make a highly readable story with a minimum of plot for there is little real detective work in the tale. Holmes is hired to try to negotiate with Milverton regarding the purchase of the letters, but to get them back no matter what! Milverton proves not only unwilling to consider a smaller amount for the papers but prepared to protect himself from Holmes attempting a search of his person. Later we learn Holmes has gotten into the household of Milverton by romancing a maid while disguised. At the end Holmes goes with Watson to burglarize Milverton's home. He and Watson are in the house when they find that Milverton is awaiting some new business deal in his study (someone with information that Milverton can use). Carefully hiding, Holmes and Watson watch as a woman comes in, who turns out to be a victim of collateral damage from Milverton's past activities, and who shoots the blackmailer to death. Holmes and Watson are able to set fire to Milverton's collection of compromising documents before fleeing the house, and subsequently discover (for themselves) the identity of the woman. The police (under Lestrade) don't discover who the two mysterious men seen running from Milverton's home are, and they are so disgusted by Milverton's activities (they never were able to bring anything home against him) that it is obvious the murder will never be solved.<br /><br />The tale is not one of the fascinating ones with real detective work involved like THE ADVENTURE OF THE SPECKLED BAND or SILVER BLAZE. It is a tale of mood and late action - the issue being will Holmes and Watson get the papers or will they be caught by Milverton? It is not one of the best stories, but it is in the bulk of the tales as being really well told and interesting.<br /><br />At the time he wrote CHARLES AUGUSTUS MILVERTON, Conan Doyle had an experience with the police regarding his sometimes activities as a highly respected amateur detective/crusader. An artist was found murdered in his studio in London, and Conan Doyle began writing his opinions about how the killing was committed. Then he stopped - apparently warned by his friends at Scotland Yard that the murder did not bare looking into. The victim had been a homosexual, and the police were certain that it was a lover's spat gone horribly wrong. For the sake of the family of the Victim (this was in 1905) Doyle dropped his interest in the case. So he was aware that sometimes the British police behaved with restraint on matters that did not seem to justify their full probing - as Lestrade's restraint towards whoever did kill the villainous Milverton in the story.<br /><br />Given the description of the story it could have been told in the normal hour long version of the series. But the teleplay for THE MASTER BLACKMAILER spent some time showing the horrible dilemma Milverton's victims (in Victorian/Edwardian England) faced. We see a promising young aristocratic army officer kill himself when faced with a homosexual exposure because of Milverton's extravagant demands, all at the start of the teleplay. And it is not only homosexuals. Men and women of good reputation in heterosexual marriages could be smeared by uncovering illegitimate children or past indiscreet relationships. Indeed, in the story, the woman who kills Milverton is avenging the destruction of her husband (a prominent nobleman) destroyed by the blackmailer. <br /><br />Milverton is well played at his most poisonous blandness by that fine actor Robert Hardy, who even when confronted by the unexpected furies he has unleashed is totally unperturbed (he looks like he will just have the angry woman showed out of his home in a moment). Brett and Hardwicke do quite well in their Holmes and Watson roles, as to be expected.<br /><br />How serious was the loss of character by rumor or innuendo in 1905? In 1898 one of the heroes of the various imperial wars, and the leader of the last victorious charge at the battle of Omdurman that destroyed the Mahdist army (see FOUR FEATHERS) was Sir Hector MacDonald. He was governor of Ceylon in 1903 when he suddenly, unexpectedly resigned. Sir Hector returned to London, and shot himself in a hotel while awaiting some sort of hearing. It later came out that \"Fighting Mac\", frequently considered the most popular army commander in Britain, had been caught having sleeping arrangements with native boys. Milverton would have eaten him up very quickly...or his real life counterparts would have.\r\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m DEBUG:train:Token IDs: tensor([  101, 11294,  2018,  2196,  2359,  2000, 24501,  3126,  2890,  6593,\r\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m          9106,  2013,  2010,  4101,  2331,  2007,  2934, 22993, 23871,  1999,\r\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m          1996,  6172,  1997,  1996,  2345,  3291,  1012,  2174,  1010,  3361,\r\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m         16852,  2081,  2032,  5627,  1006,  1999,  5775,  1007,  2000,  4339,\r\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m          1996, 19598,  1997,  1996, 19021,  5484,  3077,  2015,  1010,   102])\r\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m INFO:train:\r\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m INFO:train:======== Epoch 1 / 4 ========\r\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m INFO:train:Training...\r\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m Traceback (most recent call last):\r\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m   File \"janossch-train.py\", line 397, in <module>\r\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m     train(args)\r\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m   File \"janossch-train.py\", line 188, in train\r\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m     b_input_ids = batch[0].to(device)\r\n",
      "\u001b[36malgo-1-53b4j_1  |\u001b[0m NameError: name 'device' is not defined\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mtmpx_cw6v92_algo-1-53b4j_1 exited with code 1\n",
      "\u001b[0mAborting on container exit...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run: ['docker-compose', '-f', '/tmp/tmpx_cw6v92/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, job_name)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0m_stream_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36m_stream_output\u001b[0;34m(process)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexit_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Process exited with code: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mexit_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Process exited with code: 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-dd911a87ed0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_channel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_channel\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_for_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TrainingJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mstart_new\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   1036\u001b[0m             \u001b[0mtrain_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"enable_sagemaker_metrics\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_sagemaker_metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_job_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image, algorithm_arn, encrypt_inter_container_traffic, train_use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating training-job with name: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train request: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_training_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     def process(\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/local/local_session.py\u001b[0m in \u001b[0;36mcreate_training_job\u001b[0;34m(self, TrainingJobName, AlgorithmSpecification, OutputDataConfig, ResourceConfig, InputDataConfig, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mhyperparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"HyperParameters\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"HyperParameters\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training job\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtraining_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInputDataConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOutputDataConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingJobName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mLocalSagemakerClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_training_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTrainingJobName\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/local/entities.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, job_name)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         self.model_artifacts = self.container.train(\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0minput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         )\n\u001b[1;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, job_name)\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0;31m# which contains the exit code and append the command line to it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Failed to run: %s, %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcompose_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0martifacts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve_artifacts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompose_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to run: ['docker-compose', '-f', '/tmp/tmpx_cw6v92/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 1"
     ]
    }
   ],
   "source": [
    "estimator.fit({ \"train\": train_channel, \"test\": test_channel })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
