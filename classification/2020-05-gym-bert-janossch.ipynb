{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install huggingface transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
      "\u001b[K    100% |████████████████████████████████| 573kB 16.5MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (2.20.0)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (1.12.39)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.6)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (1.15.4)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/c1/c90beb2dbbfbf19f3634e16a441d5f11fa787bdf0748a35b8b88452c0e78/regex-2020.4.4-cp36-cp36m-manylinux1_x86_64.whl (679kB)\n",
      "\u001b[K    100% |████████████████████████████████| 686kB 24.6MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting sentencepiece (from transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/2c/8df20f3ac6c22ac224fff307ebc102818206c53fc454ecd37d8ac2060df5/sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.0MB 22.7MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting sacremoses (from transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K    100% |████████████████████████████████| 890kB 28.7MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (4.42.1)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (3.0.4)\n",
      "Collecting tokenizers==0.5.2 (from transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.7MB 10.8MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (1.23)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (2.6)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3->transformers) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.39 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3->transformers) (1.15.39)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (1.11.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (6.7)\n",
      "Collecting joblib (from sacremoses->transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl (294kB)\n",
      "\u001b[K    100% |████████████████████████████████| 296kB 27.3MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.39->boto3->transformers) (0.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.39->boto3->transformers) (2.7.3)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Running setup.py bdist_wheel for sacremoses ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built sacremoses\n",
      "\u001b[31mfastai 1.0.60 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "Installing collected packages: regex, sentencepiece, joblib, sacremoses, tokenizers, transformers\n",
      "Successfully installed joblib-0.14.1 regex-2020.4.4 sacremoses-0.0.43 sentencepiece-0.1.86 tokenizers-0.5.2 transformers-2.8.0\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting pytorch-nlp\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4f/51/f0ee1efb75f7cc2e3065c5da1363d6be2eec79691b2821594f3f2329528c/pytorch_nlp-0.5.0-py3-none-any.whl (90kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 13.8MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytorch-nlp) (1.15.4)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytorch-nlp) (4.42.1)\n",
      "\u001b[31mfastai 1.0.60 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "Installing collected packages: pytorch-nlp\n",
      "Successfully installed pytorch-nlp-0.5.0\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install pytorch-nlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch                                        # root package\n",
    "from torch.utils.data import Dataset, DataLoader    # dataset representation and loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::733425554560:role/service-role/AmazonSageMaker-ExecutionRole-20200504T094270\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Obtain dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We don't want to use the prepared dataset as it, wo we compare the sample dataset into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from torchnlp.datasets import imdb_dataset\n",
    "train, test = imdb_dataset(train=True,test=True)\n",
    "\n",
    "with open('data/train.csv', 'w') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile, delimiter=',')\n",
    "    csvwriter.writerow(['text','sentiment'])\n",
    "    for i in train:\n",
    "        csvwriter.writerow([i['text'],i['sentiment']])\n",
    "\n",
    "with open('data/test.csv', 'w') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile, delimiter=',')\n",
    "    csvwriter.writerow(['text','sentiment'])\n",
    "    for i in test:\n",
    "        csvwriter.writerow([i['text'],i['sentiment']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "mapping = {'neg': 0, 'pos': 1}\n",
    "train_df = train_df.replace({'sentiment': mapping})\n",
    "test_df = test_df.replace({'sentiment': mapping})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3460</th>\n",
       "      <td>\"Sasquatch Hunters\" actually wasn't as bad as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17539</th>\n",
       "      <td>I don't understand how \"2 of us\" receive such ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13966</th>\n",
       "      <td>Neat premise. Very unrealistic. What I learned...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9461</th>\n",
       "      <td>Does anyone else cry tears of joy when they wa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19198</th>\n",
       "      <td>There is absolutely nothing in this movie that...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>From the mind of Robert Bloch, of \"Psycho\" fam...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10375</th>\n",
       "      <td>I second the motion to make this into a movie,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15459</th>\n",
       "      <td>I caught this stink bomb of a movie recently o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9437</th>\n",
       "      <td>Visconti's first feature, Ossessione is an ada...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22616</th>\n",
       "      <td>Nightmare Weekend stars a cast of ridiculous a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  sentiment\n",
       "3460   \"Sasquatch Hunters\" actually wasn't as bad as ...          1\n",
       "17539  I don't understand how \"2 of us\" receive such ...          0\n",
       "13966  Neat premise. Very unrealistic. What I learned...          0\n",
       "9461   Does anyone else cry tears of joy when they wa...          1\n",
       "19198  There is absolutely nothing in this movie that...          0\n",
       "18     From the mind of Robert Bloch, of \"Psycho\" fam...          1\n",
       "10375  I second the motion to make this into a movie,...          1\n",
       "15459  I caught this stink bomb of a movie recently o...          0\n",
       "9437   Visconti's first feature, Ossessione is an ada...          1\n",
       "22616  Nightmare Weekend stars a cast of ridiculous a...          0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9419</th>\n",
       "      <td>Ben (a fine Charles Bateman), his young daught...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15019</th>\n",
       "      <td>This is only the fourth effort I’ve watched fr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3862</th>\n",
       "      <td>While some of the things in Haggard are dumb a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8740</th>\n",
       "      <td>First, a word of caution. The DVD box describe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9547</th>\n",
       "      <td>Multiply named and strangely casted, \"One Dark...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>I must admit that I didn't get around to seein...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24200</th>\n",
       "      <td>I love comedies and I love independent films, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22979</th>\n",
       "      <td>I'm usually quite tolerant of movies, and very...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8928</th>\n",
       "      <td>I remember this movie getting a lot of flak fr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24856</th>\n",
       "      <td>The plot was not good.&lt;br /&gt;&lt;br /&gt;The special ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  sentiment\n",
       "9419   Ben (a fine Charles Bateman), his young daught...          1\n",
       "15019  This is only the fourth effort I’ve watched fr...          0\n",
       "3862   While some of the things in Haggard are dumb a...          1\n",
       "8740   First, a word of caution. The DVD box describe...          1\n",
       "9547   Multiply named and strangely casted, \"One Dark...          1\n",
       "993    I must admit that I didn't get around to seein...          1\n",
       "24200  I love comedies and I love independent films, ...          0\n",
       "22979  I'm usually quite tolerant of movies, and very...          0\n",
       "8928   I remember this movie getting a lot of flak fr...          1\n",
       "24856  The plot was not good.<br /><br />The special ...          0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sample(int(len(train_df)*0.1))\n",
    "test_df = test_df.sample(int(len(test_df)*0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = train_df.text.values\n",
    "train_labels = train_df.sentiment.values\n",
    "test_sentences = test_df.text.values\n",
    "test_labels = test_df.sentiment.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "os.makedirs(\"./datasets/train\", exist_ok=True)\n",
    "np.save(\"./datasets/train/train_sentences.npy\", train_sentences)\n",
    "np.save(\"./datasets/train/train_labels.npy\", train_labels)\n",
    "os.makedirs(\"./datasets/test\", exist_ok=True)\n",
    "np.save(\"./datasets/test/test_sentences.npy\", test_sentences)\n",
    "np.save(\"./datasets/test/test_labels.npy\", test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = sess.default_bucket()\n",
    "PREFIX = 'bert-classification-janossch'\n",
    "\n",
    "traindata_s3_prefix = f\"{PREFIX}/datasets/train\"\n",
    "testdata_s3_prefix = f\"{PREFIX}/datasets/test\"\n",
    "output_s3 = f\"s3://{BUCKET_NAME}/{PREFIX}/models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_s3 = sess.upload_data(path=\"./datasets/train/\", bucket=BUCKET_NAME, key_prefix=traindata_s3_prefix)\n",
    "test_s3 = sess.upload_data(path=\"./datasets/test/\", bucket=BUCKET_NAME, key_prefix=testdata_s3_prefix)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TODO: remove - just exploration\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TODO: remove - just exploration\n",
    "\n",
    "# Store the model we want to use\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "\n",
    "# We need to create the model and tokenizer\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# TODO: remove - just exploration\n",
    "\n",
    "tokens = tokenizer.tokenize(train_sentences[0])\n",
    "print(\"Tokens: {}\".format(tokens))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TODO: remove - just exploration\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# TODO: remove - just exploration\n",
    "\n",
    "print('Original: ',train_sentences[0])\n",
    "print('Tokenized: ',tokenizer.tokenize(texts[0]))\n",
    "print('Token IDs: ',tokenizer.convert_tokens_to_ids(tokenizer.tokenize(train_sentences[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Dataset (moved to the training script)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TODO: remove - just exploration\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for t in train_sentences:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        t,\n",
    "                        add_special_tokens = True,\n",
    "                        max_length = 50,\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt',\n",
    "                   )\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_mask = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(train_labels)\n",
    "\n",
    "print('Original: ', train_sentences[0])\n",
    "print('Token IDs: ', input_ids[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TODO: remove - just exploration\n",
    "from transformers import BertModel\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Set the model in evaluation mode to deactivate the DropOut modules\n",
    "# This is IMPORTANT to have reproducible results during evaluation!\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU, put everything on cuda\n",
    "input_ids = input_ids.to('cuda')\n",
    "labels = labels.to('cuda')\n",
    "model.to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_channel = f\"s3://{BUCKET_NAME}/{PREFIX}/datasets/train/\"\n",
    "test_channel = f\"s3://{BUCKET_NAME}/{PREFIX}/datasets/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.estimator import PyTorch as PyTorchEstimator\n",
    "estimator = PyTorchEstimator(\n",
    "    entry_point=\"janossch-train.py\",\n",
    "    source_dir=\"src\",\n",
    "    \n",
    "    base_job_name=\"bert-classification\",\n",
    "    output_path=f\"s3://{BUCKET_NAME}/{PREFIX}/\",\n",
    "    \n",
    "    framework_version=\"1.4.0\",\n",
    "    py_version=\"py3\",\n",
    "    \n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=\"local_gpu\",\n",
    "    train_max_run=60*60,\n",
    "    train_max_wait=60*60,\n",
    "    \n",
    "    hyperparameters={\n",
    "        \"seed\": 4711,\n",
    "        \"log_level\": \"DEBUG\",\n",
    "        'batch-size': 32,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating tmp8f6bj_il_algo-1-tnt5x_1 ... \n",
      "\u001b[1BAttaching to tmp8f6bj_il_algo-1-tnt5x_12mdone\u001b[0m\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m 2020-05-06 06:31:14,852 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m 2020-05-06 06:31:14,879 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m 2020-05-06 06:31:14,882 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m 2020-05-06 06:31:15,002 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m Generating setup.py\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m 2020-05-06 06:31:15,002 sagemaker-containers INFO     Generating setup.cfg\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m 2020-05-06 06:31:15,002 sagemaker-containers INFO     Generating MANIFEST.in\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m 2020-05-06 06:31:15,003 sagemaker-containers INFO     Installing module with the following command:\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m /opt/conda/bin/python -m pip install . -r requirements.txt\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m Processing /tmp/tmppkwoujrl/module_dir\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m Collecting transformers\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m   Downloading transformers-2.8.0-py3-none-any.whl (563 kB)\n",
      "\u001b[K     |████████████████████████████████| 563 kB 16.8 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m \u001b[?25hCollecting tensorboardX\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m   Downloading tensorboardX-2.0-py2.py3-none-any.whl (195 kB)\n",
      "\u001b[K     |████████████████████████████████| 195 kB 33.9 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m \u001b[?25hCollecting tokenizers==0.5.2\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m   Downloading tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.7 MB 46.1 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m \u001b[?25hRequirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 1)) (1.12.34)\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m Collecting filelock\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m   Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m Requirement already satisfied: dataclasses; python_version < \"3.7\" in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 1)) (0.7)\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 1)) (1.16.4)\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 1)) (4.42.1)\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m Collecting sacremoses\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m   Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
      "\u001b[K     |████████████████████████████████| 883 kB 42.0 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m \u001b[?25hCollecting regex!=2019.12.17\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m   Downloading regex-2020.4.4-cp36-cp36m-manylinux2010_x86_64.whl (679 kB)\n",
      "\u001b[K     |████████████████████████████████| 679 kB 48.4 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m \u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 1)) (2.22.0)\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m Collecting sentencepiece\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m   Downloading sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 37.7 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m \u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /opt/conda/lib/python3.6/site-packages (from tensorboardX->-r requirements.txt (line 2)) (3.11.3)\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from tensorboardX->-r requirements.txt (line 2)) (1.14.0)\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers->-r requirements.txt (line 1)) (0.3.3)\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers->-r requirements.txt (line 1)) (0.9.5)\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m Requirement already satisfied: botocore<1.16.0,>=1.15.34 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers->-r requirements.txt (line 1)) (1.15.34)\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers->-r requirements.txt (line 1)) (7.1.1)\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers->-r requirements.txt (line 1)) (0.14.1)\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 1)) (1.25.8)\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 1)) (3.0.4)\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 1)) (2.8)\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 1)) (2019.11.28)\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from protobuf>=3.8.0->tensorboardX->-r requirements.txt (line 2)) (46.1.3.post20200330)\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m Requirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.34->boto3->transformers->-r requirements.txt (line 1)) (0.15.2)\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.34->boto3->transformers->-r requirements.txt (line 1)) (2.8.1)\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m Building wheels for collected packages: default-user-module-name, sacremoses\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m   Building wheel for default-user-module-name (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m \u001b[?25h  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=35208 sha256=47d73ad5469045f5df6fa7b72fda675cbed3e54fc0ac02894570e65441158195\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m   Stored in directory: /tmp/pip-ephem-wheel-cache-7b066qo8/wheels/ea/9c/fb/01b5906d2f89beec5cee6985dc82fbed971b8b5fb8d53038fb\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m   Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m \u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893259 sha256=45a9e96b61aa9266cf10bb51e1bbb89e28846ef438cde4d3dbec8c98900f9381\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m   Stored in directory: /root/.cache/pip/wheels/49/25/98/cdea9c79b2d9a22ccc59540b1784b67f06b633378e97f58da2\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m Successfully built default-user-module-name sacremoses\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m Installing collected packages: tokenizers, filelock, regex, sacremoses, sentencepiece, transformers, tensorboardX, default-user-module-name\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m Successfully installed default-user-module-name-1.0.0 filelock-3.0.12 regex-2020.4.4 sacremoses-0.0.43 sentencepiece-0.1.86 tensorboardX-2.0 tokenizers-0.5.2 transformers-2.8.0\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m 2020-05-06 06:31:20,828 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m \n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m Training Env:\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m \n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m {\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m     \"additional_framework_parameters\": {},\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m     \"channel_input_dirs\": {\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m         \"train\": \"/opt/ml/input/data/train\",\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m         \"test\": \"/opt/ml/input/data/test\"\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m     \"current_host\": \"algo-1-tnt5x\",\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m     \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m     \"hosts\": [\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m         \"algo-1-tnt5x\"\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m     ],\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m         \"seed\": 4711,\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m         \"log_level\": \"DEBUG\",\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m         \"batch-size\": 32\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m     \"input_data_config\": {\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m         \"train\": {\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m         },\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m         \"test\": {\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m         }\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m     \"job_name\": \"bert-classification-2020-05-06-06-31-10-949\",\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m     \"master_hostname\": \"algo-1-tnt5x\",\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m     \"module_dir\": \"s3://sagemaker-ap-southeast-1-733425554560/bert-classification-2020-05-06-06-31-10-949/source/sourcedir.tar.gz\",\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m     \"module_name\": \"janossch-train\",\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m     \"num_cpus\": 4,\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m     \"num_gpus\": 1,\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m         \"current_host\": \"algo-1-tnt5x\",\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m         \"hosts\": [\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m             \"algo-1-tnt5x\"\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m         ]\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m     \"user_entry_point\": \"janossch-train.py\"\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m }\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m \n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m Environment variables:\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m \n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m SM_HOSTS=[\"algo-1-tnt5x\"]\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m SM_HPS={\"batch-size\":32,\"log_level\":\"DEBUG\",\"seed\":4711}\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m SM_USER_ENTRY_POINT=janossch-train.py\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m SM_FRAMEWORK_PARAMS={}\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-tnt5x\",\"hosts\":[\"algo-1-tnt5x\"]}\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m SM_INPUT_DATA_CONFIG={\"test\":{\"TrainingInputMode\":\"File\"},\"train\":{\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m SM_CHANNELS=[\"test\",\"train\"]\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m SM_CURRENT_HOST=algo-1-tnt5x\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m SM_MODULE_NAME=janossch-train\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m SM_NUM_CPUS=4\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m SM_NUM_GPUS=1\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m SM_MODULE_DIR=s3://sagemaker-ap-southeast-1-733425554560/bert-classification-2020-05-06-06-31-10-949/source/sourcedir.tar.gz\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1-tnt5x\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1-tnt5x\"],\"hyperparameters\":{\"batch-size\":32,\"log_level\":\"DEBUG\",\"seed\":4711},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"TrainingInputMode\":\"File\"},\"train\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"bert-classification-2020-05-06-06-31-10-949\",\"log_level\":20,\"master_hostname\":\"algo-1-tnt5x\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-southeast-1-733425554560/bert-classification-2020-05-06-06-31-10-949/source/sourcedir.tar.gz\",\"module_name\":\"janossch-train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-tnt5x\",\"hosts\":[\"algo-1-tnt5x\"]},\"user_entry_point\":\"janossch-train.py\"}\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m SM_USER_ARGS=[\"--batch-size\",\"32\",\"--log_level\",\"DEBUG\",\"--seed\",\"4711\"]\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m SM_CHANNEL_TEST=/opt/ml/input/data/test\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m SM_HP_SEED=4711\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m SM_HP_LOG_LEVEL=DEBUG\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m SM_HP_BATCH-SIZE=32\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m \n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m \n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m /opt/conda/bin/python janossch-train.py --batch-size 32 --log_level DEBUG --seed 4711\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m \n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m There are 1 GPU(s) available.\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m We will use the GPU: Tesla K80\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m \n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m ======== Epoch 1 / 4 ========\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m Training...\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m   Batch    40  of     79.    Elapsed: 0:00:23.\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m \n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m   Average training loss: 0.59\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m   Training epoch took: 0:00:45\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m \n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m Running Validation...\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m   Accuracy: 0.74\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m   Validation Loss: 0.02\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m   Validation took: 0:00:14\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m \n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m ======== Epoch 2 / 4 ========\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m Training...\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m   Batch    40  of     79.    Elapsed: 0:00:23.\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m \n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m   Average training loss: 0.42\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m   Training epoch took: 0:00:45\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m \n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m Running Validation...\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m   Accuracy: 0.78\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m   Validation Loss: 0.01\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m   Validation took: 0:00:15\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m \n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m ======== Epoch 3 / 4 ========\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m Training...\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m   Batch    40  of     79.    Elapsed: 0:00:23.\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m \n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m   Average training loss: 0.29\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m   Training epoch took: 0:00:45\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m \n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m Running Validation...\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m   Accuracy: 0.79\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m   Validation Loss: 0.01\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m   Validation took: 0:00:15\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m \n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m ======== Epoch 4 / 4 ========\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m Training...\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m   Batch    40  of     79.    Elapsed: 0:00:23.\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m \n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m   Average training loss: 0.21\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m   Training epoch took: 0:00:45\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m \n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m Running Validation...\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m   Accuracy: 0.79\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m   Validation Loss: 0.02\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m   Validation took: 0:00:15\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m \n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m Training complete!\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m Total training took 0:03:58 (h:mm:ss)\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m Saving model to /opt/ml/model\n",
      "\u001b[36malgo-1-tnt5x_1  |\u001b[0m 2020-05-06 06:37:17,467 sagemaker-containers INFO     Reporting training SUCCESS\n",
      "\u001b[36mtmp8f6bj_il_algo-1-tnt5x_1 exited with code 0\n",
      "\u001b[0mAborting on container exit...\n",
      "===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({ \"train\": train_channel, \"test\": test_channel })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
