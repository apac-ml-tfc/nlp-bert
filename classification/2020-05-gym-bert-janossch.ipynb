{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install huggingface transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (3.0.4)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (2.20.0)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (2020.4.4)\n",
      "Requirement already satisfied: sentencepiece in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.1.86)\n",
      "Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (1.12.39)\n",
      "Requirement already satisfied: tokenizers==0.5.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (1.15.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (4.42.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (1.23)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (2.6)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (1.11.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (6.7)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.39 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3->transformers) (1.15.39)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3->transformers) (0.9.4)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.39->boto3->transformers) (2.7.3)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.39->boto3->transformers) (0.14)\n",
      "\u001b[31mfastai 1.0.60 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: pytorch-nlp in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (0.5.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytorch-nlp) (1.15.4)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytorch-nlp) (4.42.1)\n",
      "\u001b[31mfastai 1.0.60 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install pytorch-nlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch                                        # root package\n",
    "from torch.utils.data import Dataset, DataLoader    # dataset representation and loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::733425554560:role/service-role/AmazonSageMaker-ExecutionRole-20200504T094270\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Obtain dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We don't want to use the prepared dataset as it, wo we compare the sample dataset into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from torchnlp.datasets import imdb_dataset\n",
    "train, test = imdb_dataset(train=True,test=True)\n",
    "\n",
    "with open('data/train.csv', 'w') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile, delimiter=',')\n",
    "    csvwriter.writerow(['text','sentiment'])\n",
    "    for i in train:\n",
    "        csvwriter.writerow([i['text'],i['sentiment']])\n",
    "\n",
    "with open('data/test.csv', 'w') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile, delimiter=',')\n",
    "    csvwriter.writerow(['text','sentiment'])\n",
    "    for i in test:\n",
    "        csvwriter.writerow([i['text'],i['sentiment']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "mapping = {'neg': 0, 'pos': 1}\n",
    "train_df = train_df.replace({'sentiment': mapping})\n",
    "test_df = test_df.replace({'sentiment': mapping})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7082</th>\n",
       "      <td>The only reason I'm giving this a 9 is that th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4755</th>\n",
       "      <td>This movie is to Halloween what the hilarious ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17922</th>\n",
       "      <td>I had some expectation for the movie, since it...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20369</th>\n",
       "      <td>Having read during many years about how great ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14677</th>\n",
       "      <td>Ludicrous violations of the most basic securit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13590</th>\n",
       "      <td>Don't get me wrong, the movie is beautiful, th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9905</th>\n",
       "      <td>&lt;br /&gt;&lt;br /&gt;Arriving by boxcar in New York Cit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3211</th>\n",
       "      <td>Even though it has one of the standard \"Reveng...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16520</th>\n",
       "      <td>Tim (Gary Daniels) wants desperately to break ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8308</th>\n",
       "      <td>I can remember this movie from when i was a sm...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  sentiment\n",
       "7082   The only reason I'm giving this a 9 is that th...          1\n",
       "4755   This movie is to Halloween what the hilarious ...          1\n",
       "17922  I had some expectation for the movie, since it...          0\n",
       "20369  Having read during many years about how great ...          0\n",
       "14677  Ludicrous violations of the most basic securit...          0\n",
       "13590  Don't get me wrong, the movie is beautiful, th...          0\n",
       "9905   <br /><br />Arriving by boxcar in New York Cit...          1\n",
       "3211   Even though it has one of the standard \"Reveng...          1\n",
       "16520  Tim (Gary Daniels) wants desperately to break ...          0\n",
       "8308   I can remember this movie from when i was a sm...          1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18740</th>\n",
       "      <td>May be I don't get it right. I mean the movie....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6598</th>\n",
       "      <td>Bullets may not have bounced off his chest, bu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19165</th>\n",
       "      <td>When Family Guy first premiered, I was not in ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17192</th>\n",
       "      <td>Pretty visuals and a lot of fights make not a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15029</th>\n",
       "      <td>I'm thinking of some things for this movie: Fi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19574</th>\n",
       "      <td>When The Spirits Within was released, all you ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15814</th>\n",
       "      <td>Being a fan of the game and watching this film...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22423</th>\n",
       "      <td>Ghost Story has an interesting feminist reveng...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16956</th>\n",
       "      <td>Is this movie as bad as some claim? In my opin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4501</th>\n",
       "      <td>The movie is a starter to what really happened...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  sentiment\n",
       "18740  May be I don't get it right. I mean the movie....          0\n",
       "6598   Bullets may not have bounced off his chest, bu...          1\n",
       "19165  When Family Guy first premiered, I was not in ...          0\n",
       "17192  Pretty visuals and a lot of fights make not a ...          0\n",
       "15029  I'm thinking of some things for this movie: Fi...          0\n",
       "19574  When The Spirits Within was released, all you ...          0\n",
       "15814  Being a fan of the game and watching this film...          0\n",
       "22423  Ghost Story has an interesting feminist reveng...          0\n",
       "16956  Is this movie as bad as some claim? In my opin...          0\n",
       "4501   The movie is a starter to what really happened...          1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sample(int(len(train_df)*0.1))\n",
    "test_df = test_df.sample(int(len(test_df)*0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = train_df.text.values\n",
    "train_labels = train_df.sentiment.values\n",
    "test_sentences = test_df.text.values\n",
    "test_labels = test_df.sentiment.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "os.makedirs(\"./datasets/train\", exist_ok=True)\n",
    "np.save(\"./datasets/train/train_sentences.npy\", train_sentences)\n",
    "np.save(\"./datasets/train/train_labels.npy\", train_labels)\n",
    "os.makedirs(\"./datasets/test\", exist_ok=True)\n",
    "np.save(\"./datasets/test/test_sentences.npy\", test_sentences)\n",
    "np.save(\"./datasets/test/test_labels.npy\", test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = sess.default_bucket()\n",
    "PREFIX = 'bert-classification-janossch'\n",
    "\n",
    "traindata_s3_prefix = f\"{PREFIX}/datasets/train\"\n",
    "testdata_s3_prefix = f\"{PREFIX}/datasets/test\"\n",
    "output_s3 = f\"s3://{BUCKET_NAME}/{PREFIX}/models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_s3 = sess.upload_data(path=\"./datasets/train/\", bucket=BUCKET_NAME, key_prefix=traindata_s3_prefix)\n",
    "test_s3 = sess.upload_data(path=\"./datasets/test/\", bucket=BUCKET_NAME, key_prefix=testdata_s3_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_channel = f\"s3://{BUCKET_NAME}/{PREFIX}/datasets/train/\"\n",
    "test_channel = f\"s3://{BUCKET_NAME}/{PREFIX}/datasets/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.estimator import PyTorch as PyTorchEstimator\n",
    "estimator = PyTorchEstimator(\n",
    "    entry_point=\"janossch-train.py\",\n",
    "    source_dir=\"src\",\n",
    "    \n",
    "    base_job_name=\"bert-classification\",\n",
    "    output_path=f\"s3://{BUCKET_NAME}/{PREFIX}/\",\n",
    "    \n",
    "    framework_version=\"1.4.0\",\n",
    "    py_version=\"py3\",\n",
    "    \n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=\"local_gpu\",\n",
    "    train_max_run=60*60,\n",
    "    train_max_wait=60*60,\n",
    "    \n",
    "    hyperparameters={\n",
    "        \"seed\": 4711,\n",
    "        \"log_level\": \"DEBUG\",\n",
    "        'batch-size': 32,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating tmpot1c5y9f_algo-1-np25x_1 ... \n",
      "\u001b[1BAttaching to tmpot1c5y9f_algo-1-np25x_12mdone\u001b[0m\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m 2020-05-06 08:50:37,750 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m 2020-05-06 08:50:37,774 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m 2020-05-06 08:50:37,777 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m 2020-05-06 08:50:37,911 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m Generating setup.py\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m 2020-05-06 08:50:37,911 sagemaker-containers INFO     Generating setup.cfg\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m 2020-05-06 08:50:37,912 sagemaker-containers INFO     Generating MANIFEST.in\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m 2020-05-06 08:50:37,912 sagemaker-containers INFO     Installing module with the following command:\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m /opt/conda/bin/python -m pip install . -r requirements.txt\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m Processing /tmp/tmpplqtw111/module_dir\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m Collecting transformers\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Downloading transformers-2.8.0-py3-none-any.whl (563 kB)\n",
      "\u001b[K     |████████████████████████████████| 563 kB 10.3 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m \u001b[?25hCollecting tensorboardX\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Downloading tensorboardX-2.0-py2.py3-none-any.whl (195 kB)\n",
      "\u001b[K     |████████████████████████████████| 195 kB 28.6 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m \u001b[?25hCollecting sentencepiece\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Downloading sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 34.7 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m \u001b[?25hRequirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 1)) (1.12.34)\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m Requirement already satisfied: dataclasses; python_version < \"3.7\" in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 1)) (0.7)\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m Collecting sacremoses\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
      "\u001b[K     |████████████████████████████████| 883 kB 47.6 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m \u001b[?25hRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 1)) (4.42.1)\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 1)) (1.16.4)\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 1)) (2.22.0)\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m Collecting tokenizers==0.5.2\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Downloading tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.7 MB 46.3 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m \u001b[?25hCollecting filelock\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m Collecting regex!=2019.12.17\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Downloading regex-2020.4.4-cp36-cp36m-manylinux2010_x86_64.whl (679 kB)\n",
      "\u001b[K     |████████████████████████████████| 679 kB 35.2 MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m \u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /opt/conda/lib/python3.6/site-packages (from tensorboardX->-r requirements.txt (line 2)) (3.11.3)\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from tensorboardX->-r requirements.txt (line 2)) (1.14.0)\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers->-r requirements.txt (line 1)) (0.9.5)\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers->-r requirements.txt (line 1)) (0.3.3)\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m Requirement already satisfied: botocore<1.16.0,>=1.15.34 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers->-r requirements.txt (line 1)) (1.15.34)\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers->-r requirements.txt (line 1)) (7.1.1)\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers->-r requirements.txt (line 1)) (0.14.1)\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 1)) (2019.11.28)\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 1)) (2.8)\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 1)) (3.0.4)\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 1)) (1.25.8)\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from protobuf>=3.8.0->tensorboardX->-r requirements.txt (line 2)) (46.1.3.post20200330)\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m Requirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.34->boto3->transformers->-r requirements.txt (line 1)) (0.15.2)\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.34->boto3->transformers->-r requirements.txt (line 1)) (2.8.1)\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m Building wheels for collected packages: default-user-module-name, sacremoses\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Building wheel for default-user-module-name (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m \u001b[?25h  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=34802 sha256=9fdb00cc7978a66a2c798c363d31c9bafec390ed82cc582b04216e0f59a56501\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Stored in directory: /tmp/pip-ephem-wheel-cache-1urtikqj/wheels/dd/04/1d/f84b8ac30c807f239f02b967cb50f391d36c4f82f8430a2c28\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m \u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893259 sha256=4ec66b11cf8ef738795860b2d402d592a42e62c743a15da5016cd548168c6238\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Stored in directory: /root/.cache/pip/wheels/49/25/98/cdea9c79b2d9a22ccc59540b1784b67f06b633378e97f58da2\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m Successfully built default-user-module-name sacremoses\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m Installing collected packages: sentencepiece, regex, sacremoses, tokenizers, filelock, transformers, tensorboardX, default-user-module-name\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m Successfully installed default-user-module-name-1.0.0 filelock-3.0.12 regex-2020.4.4 sacremoses-0.0.43 sentencepiece-0.1.86 tensorboardX-2.0 tokenizers-0.5.2 transformers-2.8.0\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m 2020-05-06 08:50:43,394 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m \n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m Training Env:\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m \n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m {\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m     \"additional_framework_parameters\": {},\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m     \"channel_input_dirs\": {\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m         \"train\": \"/opt/ml/input/data/train\",\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m         \"test\": \"/opt/ml/input/data/test\"\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m     \"current_host\": \"algo-1-np25x\",\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m     \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m     \"hosts\": [\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m         \"algo-1-np25x\"\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m     ],\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m         \"seed\": 4711,\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m         \"log_level\": \"DEBUG\",\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m         \"batch-size\": 32\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m     \"input_data_config\": {\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m         \"train\": {\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m         },\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m         \"test\": {\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m         }\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m     \"job_name\": \"bert-classification-2020-05-06-08-50-34-146\",\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m     \"master_hostname\": \"algo-1-np25x\",\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m     \"module_dir\": \"s3://sagemaker-ap-southeast-1-733425554560/bert-classification-2020-05-06-08-50-34-146/source/sourcedir.tar.gz\",\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m     \"module_name\": \"janossch-train\",\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m     \"num_cpus\": 4,\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m     \"num_gpus\": 1,\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m         \"current_host\": \"algo-1-np25x\",\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m         \"hosts\": [\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m             \"algo-1-np25x\"\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m         ]\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m     \"user_entry_point\": \"janossch-train.py\"\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m }\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m \n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m Environment variables:\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m \n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m SM_HOSTS=[\"algo-1-np25x\"]\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m SM_HPS={\"batch-size\":32,\"log_level\":\"DEBUG\",\"seed\":4711}\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m SM_USER_ENTRY_POINT=janossch-train.py\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m SM_FRAMEWORK_PARAMS={}\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-np25x\",\"hosts\":[\"algo-1-np25x\"]}\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m SM_INPUT_DATA_CONFIG={\"test\":{\"TrainingInputMode\":\"File\"},\"train\":{\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m SM_CHANNELS=[\"test\",\"train\"]\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m SM_CURRENT_HOST=algo-1-np25x\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m SM_MODULE_NAME=janossch-train\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m SM_NUM_CPUS=4\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m SM_NUM_GPUS=1\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m SM_MODULE_DIR=s3://sagemaker-ap-southeast-1-733425554560/bert-classification-2020-05-06-08-50-34-146/source/sourcedir.tar.gz\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1-np25x\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1-np25x\"],\"hyperparameters\":{\"batch-size\":32,\"log_level\":\"DEBUG\",\"seed\":4711},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"TrainingInputMode\":\"File\"},\"train\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"bert-classification-2020-05-06-08-50-34-146\",\"log_level\":20,\"master_hostname\":\"algo-1-np25x\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-southeast-1-733425554560/bert-classification-2020-05-06-08-50-34-146/source/sourcedir.tar.gz\",\"module_name\":\"janossch-train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-np25x\",\"hosts\":[\"algo-1-np25x\"]},\"user_entry_point\":\"janossch-train.py\"}\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m SM_USER_ARGS=[\"--batch-size\",\"32\",\"--log_level\",\"DEBUG\",\"--seed\",\"4711\"]\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m SM_CHANNEL_TEST=/opt/ml/input/data/test\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m SM_HP_SEED=4711\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m SM_HP_LOG_LEVEL=DEBUG\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m SM_HP_BATCH-SIZE=32\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m \n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m \n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m /opt/conda/bin/python janossch-train.py --batch-size 32 --log_level DEBUG --seed 4711\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m \n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36malgo-1-np25x_1  |\u001b[0m There are 1 GPU(s) available.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m We will use the GPU: Tesla K80\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m \n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m ======== Epoch 1 / 4 ========\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m Training...\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch    40  of    782.    Elapsed: 0:00:22.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch    80  of    782.    Elapsed: 0:00:43.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   120  of    782.    Elapsed: 0:01:05.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   160  of    782.    Elapsed: 0:01:27.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   200  of    782.    Elapsed: 0:01:48.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   240  of    782.    Elapsed: 0:02:10.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   280  of    782.    Elapsed: 0:02:32.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   320  of    782.    Elapsed: 0:02:54.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   360  of    782.    Elapsed: 0:03:15.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   400  of    782.    Elapsed: 0:03:37.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   440  of    782.    Elapsed: 0:03:59.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   480  of    782.    Elapsed: 0:04:21.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   520  of    782.    Elapsed: 0:04:43.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   560  of    782.    Elapsed: 0:05:05.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   600  of    782.    Elapsed: 0:05:26.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   640  of    782.    Elapsed: 0:05:48.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   680  of    782.    Elapsed: 0:06:10.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   720  of    782.    Elapsed: 0:06:32.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   760  of    782.    Elapsed: 0:06:54.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m \n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Average training loss: 0.46\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Training epoch took: 0:07:05\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m \n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m Running Validation...\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Accuracy: 0.81\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Validation Loss: 0.01\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Validation took: 0:02:17\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m \n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m ======== Epoch 2 / 4 ========\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m Training...\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch    40  of    782.    Elapsed: 0:00:22.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch    80  of    782.    Elapsed: 0:00:44.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   120  of    782.    Elapsed: 0:01:05.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   160  of    782.    Elapsed: 0:01:27.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   200  of    782.    Elapsed: 0:01:49.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   240  of    782.    Elapsed: 0:02:11.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   280  of    782.    Elapsed: 0:02:33.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   320  of    782.    Elapsed: 0:02:55.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   360  of    782.    Elapsed: 0:03:16.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   400  of    782.    Elapsed: 0:03:38.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   440  of    782.    Elapsed: 0:04:00.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   480  of    782.    Elapsed: 0:04:22.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   520  of    782.    Elapsed: 0:04:44.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   560  of    782.    Elapsed: 0:05:06.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   600  of    782.    Elapsed: 0:05:27.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   640  of    782.    Elapsed: 0:05:49.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   680  of    782.    Elapsed: 0:06:11.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   720  of    782.    Elapsed: 0:06:33.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   760  of    782.    Elapsed: 0:06:55.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m \n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Average training loss: 0.31\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Training epoch took: 0:07:06\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m \n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m Running Validation...\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Accuracy: 0.81\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Validation Loss: 0.01\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Validation took: 0:02:17\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m \n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m ======== Epoch 3 / 4 ========\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m Training...\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch    40  of    782.    Elapsed: 0:00:22.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch    80  of    782.    Elapsed: 0:00:44.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   120  of    782.    Elapsed: 0:01:05.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   160  of    782.    Elapsed: 0:01:27.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   200  of    782.    Elapsed: 0:01:49.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   240  of    782.    Elapsed: 0:02:11.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   280  of    782.    Elapsed: 0:02:33.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   320  of    782.    Elapsed: 0:02:55.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   360  of    782.    Elapsed: 0:03:16.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   400  of    782.    Elapsed: 0:03:38.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   440  of    782.    Elapsed: 0:04:00.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   480  of    782.    Elapsed: 0:04:22.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   520  of    782.    Elapsed: 0:04:44.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   560  of    782.    Elapsed: 0:05:06.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   600  of    782.    Elapsed: 0:05:27.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   640  of    782.    Elapsed: 0:05:49.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   680  of    782.    Elapsed: 0:06:11.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   720  of    782.    Elapsed: 0:06:33.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   760  of    782.    Elapsed: 0:06:55.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m \n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Average training loss: 0.19\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Training epoch took: 0:07:06\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m \n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m Running Validation...\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Accuracy: 0.82\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Validation Loss: 0.02\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Validation took: 0:02:17\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m \n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m ======== Epoch 4 / 4 ========\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m Training...\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch    40  of    782.    Elapsed: 0:00:22.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch    80  of    782.    Elapsed: 0:00:44.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   120  of    782.    Elapsed: 0:01:05.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   160  of    782.    Elapsed: 0:01:27.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   200  of    782.    Elapsed: 0:01:49.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   240  of    782.    Elapsed: 0:02:11.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   280  of    782.    Elapsed: 0:02:33.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   320  of    782.    Elapsed: 0:02:55.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   360  of    782.    Elapsed: 0:03:16.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   400  of    782.    Elapsed: 0:03:38.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   440  of    782.    Elapsed: 0:04:00.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   480  of    782.    Elapsed: 0:04:22.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   520  of    782.    Elapsed: 0:04:44.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   560  of    782.    Elapsed: 0:05:06.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   600  of    782.    Elapsed: 0:05:28.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   640  of    782.    Elapsed: 0:05:49.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   680  of    782.    Elapsed: 0:06:11.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   720  of    782.    Elapsed: 0:06:33.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Batch   760  of    782.    Elapsed: 0:06:55.\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m \n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Average training loss: 0.12\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Training epoch took: 0:07:07\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m \n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m Running Validation...\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Accuracy: 0.82\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Validation Loss: 0.02\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m   Validation took: 0:02:17\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m \n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m Training complete!\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m Total training took 0:37:33 (h:mm:ss)\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m Saving model to /opt/ml/model\n",
      "\u001b[36malgo-1-np25x_1  |\u001b[0m 2020-05-06 09:34:47,858 sagemaker-containers INFO     Reporting training SUCCESS\n",
      "\u001b[36mtmpot1c5y9f_algo-1-np25x_1 exited with code 0\n",
      "\u001b[0mAborting on container exit...\n",
      "===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({ \"train\": train_channel, \"test\": test_channel })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
