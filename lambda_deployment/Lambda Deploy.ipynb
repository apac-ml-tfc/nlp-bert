{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying HuggingFace BERT on AWS Lambda\n",
    "\n",
    "The main challenge of deploying HuggingFace BERT-based models to AWS Lambda is space - per the [Lambda documentation](https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html), the following limits apply:\n",
    "\n",
    "- Maximum deployment package size (including dependencies): 50MB zipped; 250MB unzipped\n",
    "- `/tmp` directory storage: 512MB\n",
    "- Maximum RAM allocation: 3,008MB\n",
    "\n",
    "This is tricky because a typical PyTorch+Transformers installation will easily exceed 250MB (and in fact can be over 512MB too); and a trained BERT model itself may add another couple of hundred MB of data.\n",
    "\n",
    "The solution therefore requires us to get a bit creative with our use of storage - and this will come at the cost of latency. BERT-based models are typically resource-intensive anyway, so this example will be relevant to specific use-cases and not normally a preferred deployment pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and configuration\n",
    "\n",
    "As usual, we'll first load and connect to our SDKs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For easier dev of local modules:\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Python Built-Ins:\n",
    "import json\n",
    "import os\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "import sagemaker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"2020-05-gym-bert\"\n",
    "%store BUCKET_NAME\n",
    "\n",
    "SQUAD_V2 = False  # Whether to use V2 (including unanswerable questions)\n",
    "%store SQUAD_V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "botosess = boto3.session.Session()\n",
    "region = botosess.region_name\n",
    "s3 = botosess.resource(\"s3\")\n",
    "bucket = s3.Bucket(BUCKET_NAME)\n",
    "smclient = botosess.client(\"sagemaker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch - IGNOREME\n",
    "\n",
    "You don't actually need to download and inspect your model tarballs... they're already in S3 from the SageMaker training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket.Object(\n",
    "    \"bert-calssification-distributed-2020-05-05-15-58-03-622/output/output.tar.gz\"\n",
    ").download_file(\n",
    "    \"models/bert-cls.tar.gz\"\n",
    ")\n",
    "bucket.Object(\n",
    "    \"distilbert-calssification-distributed-2020-05-05-16-24-55-728/output/output.tar.gz\"\n",
    ").download_file(\n",
    "    \"models/distilbert-cls.tar.gz\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p models/bert-cls\n",
    "!tar -C models/bert-cls -zxvf models/bert-cls.tar.gz\n",
    "!mkdir -p models/distilbert-cls\n",
    "!tar -C models/distilbert-cls -zxvf models/distilbert-cls.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install AWS SAM (via Brew)\n",
    "\n",
    "In this example we'll create our Lambda function with an API Gateway deployment, via a CloudFormation template. AWS SAM CLI will simplify defining the API deployment, and allow us to build the Lambda function in a nice, reproducible Docker environment.\n",
    "\n",
    "This script is designed to be run on a SageMaker notebook instance. If you're on a local machine with SAM already installed, you can skip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Maybe factor the .sh into the notebook with %%sh when it's stable\n",
    "# (At the moment it's convenient to call it either via terminal or notebook though)\n",
    "\n",
    "# Install AWS SAM\n",
    "!./install-sam.sh\n",
    "\n",
    "# The script should add SAM to PATH anyway, but this Kernel is a parent process so we'll have to replicate:\n",
    "os.environ[\"PATH\"] += \"/home/linuxbrew/.linuxbrew/bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check SAM's installed and visible:\n",
    "!sam --version\n",
    "\n",
    "# FIXME: It isn't! Grr... This works though:\n",
    "!source ~/.profile && sam --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install function dependencies and create Lambda package\n",
    "\n",
    "Because we need to optimize the way our dependencies are loaded into the Lambda, the standard SAM build requirements.txt method of specifying libraries won't cut it.\n",
    "\n",
    "We'll install our requirements on a (conda) virtual environment, and copy them in to the bundle.\n",
    "\n",
    "**To add extra dependencnies, modify [configure-venv.sh](configure-venv.sh)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optionally run this to clear existing env, since the below script re-uses existing:\n",
    "!conda env remove -n lambda_bert -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create an empty virtual env, install dependencies, then extract it into lambda/packages\n",
    "!./configure-venv.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Lambda package\n",
    "\n",
    "Check our unzipped Lambda bundle and the contents we'll extract to /tmp are within the size limits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "du -sh lambda/build  # Must be under 250MB\n",
    "rm -rf lambda/packages-tmp-sizecheck\n",
    "unzip -q -d lambda/packages-tmp-sizecheck lambda/build/packages-tmpdir.zip\n",
    "du -sh lambda/packages-tmp-sizecheck  # Must be under 512MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy\n",
    "\n",
    "Now our raw Lambda source code (from [lambda/src](lambda/src)) and the libraries we need (from conda env `lambda_bert`) have been packaged together (in [lambda/build](lambda/build)).\n",
    "\n",
    "We're ready to build and deploy our SAM-based serverless application stack:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Deploy the Lambda + API Gateway:\n",
    "STAGING_BUCKET = \"2020-05-gym-bert-sam-staging\"\n",
    "STACK_NAME = \"test\"\n",
    "\n",
    "# FIXME: Figure how to get brew on the kernel's path properly so source ~/.profile isn't needed\n",
    "!source ~/.profile && ./deploy.sh {STAGING_BUCKET} {STACK_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "TODO\n",
    "\n",
    "For now, just GET /invoke on the APIEndpoint output above by the stack creation - e.g. in your browser.\n",
    "\n",
    "If everything is \"working\", probably first call will time out and second call will give a generic howdy response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
